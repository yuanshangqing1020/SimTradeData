"""
å¢é‡åŒæ­¥å™¨

è´Ÿè´£æ£€æµ‹æœ€åæ•°æ®æ—¥æœŸï¼Œè®¡ç®—å¢é‡æ•°æ®èŒƒå›´ï¼Œæ‰§è¡Œæ‰¹é‡æ•°æ®åŒæ­¥ã€‚
"""

import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import date, datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from ..config import Config
from ..data_sources import DataSourceManager
from ..database import DatabaseManager
from ..preprocessor import DataProcessingEngine

logger = logging.getLogger(__name__)


class IncrementalSync:
    """å¢é‡åŒæ­¥å™¨"""

    def __init__(
        self,
        db_manager: DatabaseManager,
        data_source_manager: DataSourceManager,
        processing_engine: DataProcessingEngine,
        config: Config = None,
    ):
        """
        åˆå§‹åŒ–å¢é‡åŒæ­¥å™¨

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            data_source_manager: æ•°æ®æºç®¡ç†å™¨
            preprocessor: æ•°æ®é¢„å¤„ç†å™¨
            config: é…ç½®å¯¹è±¡
        """
        self.db_manager = db_manager
        self.data_source_manager = data_source_manager
        self.processing_engine = processing_engine
        self.preprocessor = processing_engine  # å…¼å®¹æ€§åˆ«å
        self.config = config or Config()

        # åŒæ­¥é…ç½®
        self.max_sync_days = self.config.get("sync.max_sync_days", 30)
        self.batch_size = self.config.get("sync.batch_size", 50)
        self.max_workers = self.config.get("sync.max_workers", 3)
        self.sync_frequencies = self.config.get("sync.frequencies", ["1d"])
        self.enable_parallel = self.config.get("sync.enable_parallel", True)

        # æ™ºèƒ½è¡¥å……é…ç½®
        self.enable_smart_backfill = self.config.get("sync.enable_smart_backfill", True)
        self.backfill_batch_size = self.config.get("sync.backfill_batch_size", 50)
        self.backfill_sample_size = self.config.get("sync.backfill_sample_size", 10)

        # åŒæ­¥ç»Ÿè®¡
        self.sync_stats = {
            "total_symbols": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_date_ranges": {},
            "errors": [],
        }

        logger.info("å¢é‡åŒæ­¥å™¨åˆå§‹åŒ–å®Œæˆ")

    def sync_all_symbols(
        self,
        target_date: date = None,
        symbols: List[str] = None,
        frequencies: List[str] = None,
        progress_bar=None,
    ) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰è‚¡ç¥¨çš„å¢é‡æ•°æ®

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ´»è·ƒè‚¡ç¥¨
            frequencies: é¢‘ç‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºé…ç½®ä¸­çš„é¢‘ç‡

        Returns:
            Dict[str, Any]: åŒæ­¥ç»“æœ
        """
        if target_date is None:
            target_date = datetime.now().date()

        if frequencies is None:
            frequencies = self.sync_frequencies

        try:
            logger.info(f"å¼€å§‹å¢é‡åŒæ­¥: ç›®æ ‡æ—¥æœŸ={target_date}, é¢‘ç‡={frequencies}")

            # é‡ç½®ç»Ÿè®¡
            self._reset_stats()

            # è·å–éœ€è¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                symbols = self._get_active_symbols()

            self.sync_stats["total_symbols"] = len(symbols)

            # ğŸš€ æ™ºèƒ½è¡¥å……é˜¶æ®µï¼šæ£€æŸ¥å¹¶è¡¥å……å†å²æ•°æ®çš„è¡ç”Ÿå­—æ®µ
            backfill_stats = {
                "enabled": self.enable_smart_backfill,
                "checked_symbols": 0,
                "needs_backfill_symbols": 0,
                "backfilled_symbols": 0,
                "backfilled_records": 0,
                "backfill_errors": 0,
            }

            if self.enable_smart_backfill:
                logger.info("å¼€å§‹æ™ºèƒ½æ•°æ®è´¨é‡æ£€æŸ¥å’Œè¡¥å……...")

                # æ£€æŸ¥å‰å‡ åªè‚¡ç¥¨æ¥ä¼°ç®—æ•´ä½“æƒ…å†µ
                sample_size = min(self.backfill_sample_size, len(symbols))
                sample_symbols = symbols[:sample_size]
                needs_backfill_count = 0

                for symbol in sample_symbols:
                    quality_check = self.check_data_quality(symbol, frequencies[0])
                    if quality_check.get("needs_backfill", False):
                        needs_backfill_count += 1

                # å¦‚æœæ ·æœ¬ä¸­æœ‰éœ€è¦è¡¥å……çš„æ•°æ®ï¼Œåˆ™å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œæ™ºèƒ½è¡¥å……
                if needs_backfill_count > 0:
                    backfill_ratio = needs_backfill_count / sample_size
                    estimated_total = int(len(symbols) * backfill_ratio)
                    logger.info(
                        f"æ£€æµ‹åˆ°æ•°æ®è´¨é‡é—®é¢˜ï¼šæ ·æœ¬ä¸­ {needs_backfill_count}/{sample_size} åªè‚¡ç¥¨éœ€è¦è¡¥å……è¡ç”Ÿå­—æ®µ"
                    )
                    logger.info(
                        f"é¢„ä¼°å…¨éƒ¨ {len(symbols)} åªè‚¡ç¥¨ä¸­çº¦ {estimated_total} åªéœ€è¦è¡¥å……ï¼Œå¼€å§‹æ™ºèƒ½è¡¥å……..."
                    )

                    # å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œæ™ºèƒ½è¡¥å……ï¼ˆåˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜é—®é¢˜ï¼‰
                    batch_size = self.backfill_batch_size

                    for i in range(0, len(symbols), batch_size):
                        batch_symbols = symbols[i : i + batch_size]
                        batch_num = i // batch_size + 1
                        total_batches = (len(symbols) + batch_size - 1) // batch_size

                        logger.info(
                            f"æ™ºèƒ½è¡¥å……æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç† {len(batch_symbols)} åªè‚¡ç¥¨"
                        )

                        for symbol in batch_symbols:
                            try:
                                # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å……
                                quality_check = self.check_data_quality(
                                    symbol, frequencies[0]
                                )
                                backfill_stats["checked_symbols"] += 1

                                if quality_check.get("needs_backfill", False):
                                    backfill_stats["needs_backfill_symbols"] += 1

                                    # æ‰§è¡Œæ™ºèƒ½è¡¥å……
                                    backfill_result = self.smart_backfill_symbol(
                                        symbol, frequencies[0]
                                    )

                                    if backfill_result.get("success", False):
                                        backfill_stats["backfilled_symbols"] += 1
                                        backfill_stats[
                                            "backfilled_records"
                                        ] += backfill_result.get("updated_count", 0)
                                    else:
                                        backfill_stats["backfill_errors"] += 1

                            except Exception as e:
                                logger.warning(f"æ™ºèƒ½è¡¥å……è‚¡ç¥¨ {symbol} æ—¶å‡ºé”™: {e}")
                                backfill_stats["backfill_errors"] += 1

                        # æ›´æ–°è¿›åº¦æ¡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                        if progress_bar:
                            progress_bar.update(len(batch_symbols))

                    logger.info(
                        f"æ™ºèƒ½è¡¥å……å®Œæˆ: æ£€æŸ¥äº† {backfill_stats['checked_symbols']} åªè‚¡ç¥¨ï¼Œ"
                        f"è¡¥å……äº† {backfill_stats['backfilled_symbols']} åªè‚¡ç¥¨çš„ {backfill_stats['backfilled_records']} æ¡è®°å½•"
                    )
                else:
                    logger.info("æ ·æœ¬æ£€æŸ¥æ˜¾ç¤ºæ•°æ®è´¨é‡è‰¯å¥½ï¼Œè·³è¿‡æ™ºèƒ½è¡¥å……é˜¶æ®µ")
            else:
                logger.info("æ™ºèƒ½è¡¥å……åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ•°æ®è´¨é‡æ£€æŸ¥")

            # ğŸ“ˆ æ­£å¸¸å¢é‡åŒæ­¥é˜¶æ®µ

            # æŒ‰é¢‘ç‡åŒæ­¥
            for frequency in frequencies:
                freq_result = self._sync_frequency_data(
                    symbols, target_date, frequency, progress_bar
                )
                self.sync_stats["sync_date_ranges"][frequency] = freq_result

            # æ›´æ–°åŒæ­¥çŠ¶æ€
            self._update_sync_status(target_date, self.sync_stats)

            # å°†æ™ºèƒ½è¡¥å……ç»Ÿè®¡ä¿¡æ¯æ·»åŠ åˆ°ç»“æœä¸­
            self.sync_stats["smart_backfill"] = backfill_stats

            # ä¿®æ­£æˆåŠŸç‡è®¡ç®—ï¼šå¦‚æœæœ‰æˆåŠŸçš„è‚¡ç¥¨å¤„ç†ï¼Œå³ä½¿æœ‰éƒ¨åˆ†é”™è¯¯ä¹Ÿåº”å½“æ•´ä½“æ ‡è®°ä¸ºéƒ¨åˆ†æˆåŠŸ
            effective_success = self.sync_stats["success_count"] > 0
            effective_error = self.sync_stats["error_count"] > 0

            if effective_success and not effective_error:
                result_status = "completed"
            elif effective_success and effective_error:
                result_status = "partial_success"
            else:
                result_status = "failed"

            logger.info(
                f"å¢é‡åŒæ­¥å®Œæˆ: æˆåŠŸ={self.sync_stats['success_count']}, "
                f"é”™è¯¯={self.sync_stats['error_count']}, "
                f"è·³è¿‡={self.sync_stats['skipped_count']}, "
                f"æ•´ä½“çŠ¶æ€={result_status}"
            )

            if backfill_stats["backfilled_symbols"] > 0:
                logger.info(
                    f"æ™ºèƒ½è¡¥å……å®Œæˆ: è¡¥å……äº† {backfill_stats['backfilled_symbols']} åªè‚¡ç¥¨çš„ "
                    f"{backfill_stats['backfilled_records']} æ¡å†å²è®°å½•çš„è¡ç”Ÿå­—æ®µ"
                )

            return self.sync_stats.copy()

        except Exception as e:
            logger.error(f"å¢é‡åŒæ­¥å¤±è´¥: {e}")
            raise

    def check_data_quality(self, symbol: str, frequency: str = "1d") -> Dict[str, Any]:
        """
        æ£€æŸ¥è‚¡ç¥¨æ•°æ®è´¨é‡ï¼Œç‰¹åˆ«æ˜¯è¡ç”Ÿå­—æ®µçš„å®Œæ•´æ€§

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Dict[str, Any]: æ•°æ®è´¨é‡æŠ¥å‘Š
        """
        try:
            sql = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(CASE WHEN change_percent IS NULL THEN 1 END) as null_change_percent,
                COUNT(CASE WHEN prev_close IS NULL THEN 1 END) as null_prev_close,
                COUNT(CASE WHEN amplitude IS NULL THEN 1 END) as null_amplitude,
                COUNT(CASE WHEN source LIKE '%enhanced' THEN 1 END) as enhanced_records,
                MIN(date) as earliest_date,
                MAX(date) as latest_date
            FROM market_data 
            WHERE symbol = ? AND frequency = ?
            """

            result = self.db_manager.fetchone(sql, (symbol, frequency))

            if result:
                total = result["total_records"]
                null_derived = result["null_change_percent"]

                return {
                    "symbol": symbol,
                    "total_records": total,
                    "null_change_percent": null_derived,
                    "null_prev_close": result["null_prev_close"],
                    "null_amplitude": result["null_amplitude"],
                    "enhanced_records": result["enhanced_records"],
                    "earliest_date": result["earliest_date"],
                    "latest_date": result["latest_date"],
                    "needs_backfill": null_derived > 0,
                    "backfill_ratio": null_derived / total if total > 0 else 0,
                }
            else:
                return {
                    "symbol": symbol,
                    "total_records": 0,
                    "needs_backfill": False,
                    "backfill_ratio": 0,
                }

        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®è´¨é‡å¤±è´¥ {symbol}: {e}")
            return {
                "symbol": symbol,
                "total_records": 0,
                "needs_backfill": False,
                "error": str(e),
            }

    def smart_backfill_symbol(
        self, symbol: str, frequency: str = "1d"
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½è¡¥å……å•ä¸ªè‚¡ç¥¨çš„è¡ç”Ÿå­—æ®µæ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Dict[str, Any]: è¡¥å……ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½è¡¥å……è‚¡ç¥¨æ•°æ®: {symbol}")

            # è·å–è¯¥è‚¡ç¥¨çš„æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ—¥æœŸæ’åº
            data_sql = """
            SELECT date, open, high, low, close, volume, amount
            FROM market_data 
            WHERE symbol = ? AND frequency = ?
            ORDER BY date
            """

            records = self.db_manager.fetchall(data_sql, (symbol, frequency))

            if not records:
                return {
                    "symbol": symbol,
                    "success": False,
                    "updated_count": 0,
                    "message": "æ— æ•°æ®è®°å½•",
                }

            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œæ‰¹é‡è®¡ç®—
            import numpy as np
            import pandas as pd

            df = pd.DataFrame([dict(record) for record in records])
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date")

            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloatç±»å‹
            numeric_columns = ["open", "high", "low", "close", "volume", "amount"]
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

            # è®¡ç®—å‰ä¸€æ—¥æ”¶ç›˜ä»·
            df["prev_close_new"] = df["close"].shift(1)

            # è®¡ç®—æ¶¨è·Œé¢
            df["change_amount_new"] = df["close"] - df["prev_close_new"]

            # è®¡ç®—æ¶¨è·Œå¹…ï¼ˆç™¾åˆ†æ¯”ï¼‰
            df["change_percent_new"] = np.where(
                df["prev_close_new"] > 0,
                (df["change_amount_new"] / df["prev_close_new"] * 100).round(4),
                0.0,
            )

            # è®¡ç®—æŒ¯å¹…
            df["amplitude_new"] = np.where(
                df["prev_close_new"] > 0,
                ((df["high"] - df["low"]) / df["prev_close_new"] * 100).round(4),
                0.0,
            )

            # è®¡ç®—æ¶¨è·Œåœä»·æ ¼
            df["high_limit_new"] = np.where(
                df["prev_close_new"] > 0, (df["prev_close_new"] * 1.1).round(2), None
            )
            df["low_limit_new"] = np.where(
                df["prev_close_new"] > 0, (df["prev_close_new"] * 0.9).round(2), None
            )

            # åˆ¤æ–­æ¶¨åœè·Œåœ
            df["is_limit_up_new"] = False
            df["is_limit_down_new"] = False

            valid_high_limit = df["high_limit_new"].notna()
            valid_low_limit = df["low_limit_new"].notna()

            if valid_high_limit.any():
                df.loc[valid_high_limit, "is_limit_up_new"] = (
                    df.loc[valid_high_limit, "close"]
                    >= df.loc[valid_high_limit, "high_limit_new"]
                )
            if valid_low_limit.any():
                df.loc[valid_low_limit, "is_limit_down_new"] = (
                    df.loc[valid_low_limit, "close"]
                    <= df.loc[valid_low_limit, "low_limit_new"]
                )

            # ç¬¬ä¸€è¡Œè®¾ä¸ºé»˜è®¤å€¼
            if len(df) > 0:
                first_idx = df.index[0]
                df.loc[
                    first_idx,
                    [
                        "prev_close_new",
                        "change_amount_new",
                        "change_percent_new",
                        "amplitude_new",
                        "high_limit_new",
                        "low_limit_new",
                        "is_limit_up_new",
                        "is_limit_down_new",
                    ],
                ] = [None, 0.0, 0.0, 0.0, None, None, False, False]

            # æ‰¹é‡æ›´æ–°æ•°æ®åº“
            updated_count = 0
            update_sql = """
            UPDATE market_data 
            SET prev_close = ?, change_amount = ?, change_percent = ?, amplitude = ?,
                high_limit = ?, low_limit = ?, is_limit_up = ?, is_limit_down = ?,
                source = CASE WHEN source LIKE '%enhanced' THEN source ELSE 'smart_backfilled_enhanced' END,
                quality_score = 100
            WHERE symbol = ? AND date = ? AND frequency = ?
            """

            for _, row in df.iterrows():
                try:
                    params = (
                        (
                            row["prev_close_new"]
                            if pd.notna(row["prev_close_new"])
                            else None
                        ),
                        (
                            row["change_amount_new"]
                            if pd.notna(row["change_amount_new"])
                            else 0.0
                        ),
                        (
                            row["change_percent_new"]
                            if pd.notna(row["change_percent_new"])
                            else 0.0
                        ),
                        row["amplitude_new"] if pd.notna(row["amplitude_new"]) else 0.0,
                        (
                            row["high_limit_new"]
                            if pd.notna(row["high_limit_new"])
                            else None
                        ),
                        (
                            row["low_limit_new"]
                            if pd.notna(row["low_limit_new"])
                            else None
                        ),
                        bool(row["is_limit_up_new"]),
                        bool(row["is_limit_down_new"]),
                        symbol,
                        row["date"].strftime("%Y-%m-%d"),
                        frequency,
                    )

                    self.db_manager.execute(update_sql, params)
                    updated_count += 1

                except Exception as e:
                    logger.warning(f"æ›´æ–°è®°å½•å¤±è´¥ {symbol} {row['date']}: {e}")

            logger.info(f"æ™ºèƒ½è¡¥å……å®Œæˆ {symbol}: æ›´æ–° {updated_count} æ¡è®°å½•")

            return {
                "symbol": symbol,
                "success": True,
                "updated_count": updated_count,
                "total_records": len(df),
                "message": f"æˆåŠŸæ›´æ–° {updated_count} æ¡è®°å½•",
            }

        except Exception as e:
            logger.error(f"æ™ºèƒ½è¡¥å……å¤±è´¥ {symbol}: {e}")
            return {
                "symbol": symbol,
                "success": False,
                "updated_count": 0,
                "error": str(e),
            }

    def sync_symbol_range(
        self, symbol: str, start_date: date, end_date: date, frequency: str = "1d"
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å•ä¸ªè‚¡ç¥¨çš„æ—¥æœŸèŒƒå›´æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            frequency: é¢‘ç‡

        Returns:
            Dict[str, Any]: åŒæ­¥ç»“æœ
        """
        try:
            logger.info(
                f"åŒæ­¥è‚¡ç¥¨èŒƒå›´æ•°æ®: {symbol} {start_date} åˆ° {end_date} {frequency}"
            )

            result = {
                "symbol": symbol,
                "start_date": str(start_date),
                "end_date": str(end_date),
                "frequency": frequency,
                "success_count": 0,
                "error_count": 0,
                "sync_dates": [],
            }

            # æ‰¹é‡åŒæ­¥æ•´ä¸ªæ—¥æœŸèŒƒå›´
            try:
                # ä½¿ç”¨æ•°æ®å¤„ç†å¼•æ“æ‰¹é‡å¤„ç†æ—¥æœŸèŒƒå›´æ•°æ®
                process_result = self.processing_engine.process_stock_data(
                    symbol, start_date, end_date, frequency, force_update=True
                )

                # è§£æåµŒå¥—çš„è¿”å›ç»“æœæ ¼å¼
                actual_result = process_result
                if isinstance(process_result, dict) and "data" in process_result:
                    actual_result = process_result["data"]
                    if isinstance(actual_result, dict) and "data" in actual_result:
                        actual_result = actual_result["data"]

                # ç»Ÿè®¡ç»“æœ
                result["success_count"] = len(actual_result.get("processed_dates", []))
                result["error_count"] = len(actual_result.get("failed_dates", []))
                result["sync_dates"] = actual_result.get("processed_dates", [])

                logger.debug(
                    f"æ‰¹é‡å¤„ç†ç»“æœ: æˆåŠŸ={result['success_count']}, å¤±è´¥={result['error_count']}"
                )

            except Exception as e:
                logger.error(f"æ‰¹é‡åŒæ­¥å¤±è´¥ {symbol} {start_date}-{end_date}: {e}")
                result["error_count"] = 1

            logger.info(
                f"è‚¡ç¥¨èŒƒå›´åŒæ­¥å®Œæˆ: {symbol}, æˆåŠŸ={result['success_count']}, "
                f"é”™è¯¯={result['error_count']}"
            )

            return result

        except Exception as e:
            logger.error(f"è‚¡ç¥¨èŒƒå›´åŒæ­¥å¤±è´¥ {symbol}: {e}")
            raise

    def get_last_data_date(self, symbol: str, frequency: str = "1d") -> Optional[date]:
        """
        è·å–è‚¡ç¥¨çš„æœ€åæ•°æ®æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Optional[date]: æœ€åæ•°æ®æ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰æ•°æ®åˆ™è¿”å›None
        """
        try:
            sql = """
            SELECT MAX(date) as last_date
            FROM market_data
            WHERE symbol = ? AND frequency = ?
            """

            result = self.db_manager.fetchone(sql, (symbol, frequency))

            if result and result["last_date"]:
                return datetime.strptime(result["last_date"], "%Y-%m-%d").date()
            else:
                return None

        except Exception as e:
            logger.error(f"è·å–æœ€åæ•°æ®æ—¥æœŸå¤±è´¥ {symbol}: {e}")
            return None

    def calculate_sync_range(
        self, symbol: str, target_date: date, frequency: str = "1d"
    ) -> Tuple[Optional[date], date]:
        """
        è®¡ç®—å¢é‡åŒæ­¥çš„æ—¥æœŸèŒƒå›´

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            target_date: ç›®æ ‡æ—¥æœŸ
            frequency: é¢‘ç‡

        Returns:
            Tuple[Optional[date], date]: (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ)
        """
        try:
            # è·å–æœ€åæ•°æ®æ—¥æœŸ
            last_date = self.get_last_data_date(symbol, frequency)

            if last_date is None:
                # æ²¡æœ‰å†å²æ•°æ®ï¼Œä»é…ç½®çš„é»˜è®¤å¼€å§‹æ—¥æœŸåŒæ­¥
                default_start = self.config.get("sync.default_start_date", "2020-01-01")
                start_date = datetime.strptime(default_start, "%Y-%m-%d").date()

                # é™åˆ¶æœ€å¤§åŒæ­¥å¤©æ•°
                max_start = target_date - timedelta(days=self.max_sync_days)
                if start_date < max_start:
                    start_date = max_start

                logger.info(f"é¦–æ¬¡åŒæ­¥ {symbol}: {start_date} åˆ° {target_date}")
                return start_date, target_date
            else:
                # æœ‰å†å²æ•°æ®ï¼Œä»æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹
                start_date = last_date + timedelta(days=1)

                # æ£€æŸ¥æ˜¯å¦å°è¯•åŒæ­¥æœªæ¥æ—¥æœŸ
                today = datetime.now().date()
                if target_date > today:
                    target_date = today
                    logger.debug(f"ç›®æ ‡æ—¥æœŸè°ƒæ•´ä¸ºä»Šå¤©: {target_date}")

                if start_date > target_date:
                    # å·²ç»æ˜¯æœ€æ–°æ•°æ®
                    logger.debug(f"æ•°æ®å·²æ˜¯æœ€æ–° {symbol}: æœ€åæ—¥æœŸ={last_date}")
                    return None, target_date

                logger.debug(f"å¢é‡åŒæ­¥ {symbol}: {start_date} åˆ° {target_date}")
                return start_date, target_date

        except Exception as e:
            logger.error(f"è®¡ç®—åŒæ­¥èŒƒå›´å¤±è´¥ {symbol}: {e}")
            return None, target_date

    def _sync_frequency_data(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """åŒæ­¥ç‰¹å®šé¢‘ç‡çš„æ•°æ®"""
        logger.info(f"åŒæ­¥é¢‘ç‡æ•°æ®: {frequency}, è‚¡ç¥¨æ•°é‡: {len(symbols)}")

        result = {
            "frequency": frequency,
            "total_symbols": len(symbols),
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        # ä½¿ç”¨æµæ°´çº¿æ¨¡å¼ï¼šä¸‹è½½ä¸²è¡Œï¼Œå¤„ç†å¹¶å‘
        result.update(
            self._sync_pipeline(symbols, target_date, frequency, progress_bar)
        )

        # æ›´æ–°æ€»ç»Ÿè®¡
        self.sync_stats["success_count"] += result["success_count"]
        self.sync_stats["error_count"] += result["error_count"]
        self.sync_stats["skipped_count"] += result["skipped_count"]

        return result

    def _sync_sequential(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """ä¸²è¡ŒåŒæ­¥"""
        result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        for symbol in symbols:
            try:
                # è®¡ç®—åŒæ­¥èŒƒå›´
                start_date, end_date = self.calculate_sync_range(
                    symbol, target_date, frequency
                )

                if start_date is None:
                    result["skipped_count"] += 1
                    # æ›´æ–°è¿›åº¦æ¡
                    if progress_bar:
                        progress_bar.update(1)
                    continue

                # åŒæ­¥æ•°æ®
                sync_result = self.sync_symbol_range(
                    symbol, start_date, end_date, frequency
                )

                if sync_result["success_count"] > 0:
                    result["success_count"] += 1
                    result["sync_ranges"][symbol] = {
                        "start_date": str(start_date),
                        "end_date": str(end_date),
                        "sync_count": sync_result["success_count"],
                    }
                else:
                    result["error_count"] += 1

                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    progress_bar.update(1)

            except Exception as e:
                logger.error(f"ä¸²è¡ŒåŒæ­¥å¤±è´¥ {symbol}: {e}")
                result["error_count"] += 1
                self.sync_stats["errors"].append(
                    {"symbol": symbol, "frequency": frequency, "error": str(e)}
                )
                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    progress_bar.update(1)

        return result

    def _sync_parallel(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """å¹¶è¡ŒåŒæ­¥"""
        result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        # åˆ†æ‰¹å¤„ç†
        symbol_batches = [
            symbols[i : i + self.batch_size]
            for i in range(0, len(symbols), self.batch_size)
        ]

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_batch = {
                executor.submit(
                    self._sync_symbol_batch, batch, target_date, frequency
                ): batch
                for batch in symbol_batches
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    batch_result = future.result()
                    result["success_count"] += batch_result["success_count"]
                    result["error_count"] += batch_result["error_count"]
                    result["skipped_count"] += batch_result["skipped_count"]
                    result["sync_ranges"].update(batch_result["sync_ranges"])

                except Exception as e:
                    logger.error(f"å¹¶è¡ŒåŒæ­¥æ‰¹æ¬¡å¤±è´¥: {batch}, é”™è¯¯: {e}")
                    result["error_count"] += len(batch)

        return result

    def _sync_symbol_batch(
        self, symbols: List[str], target_date: date, frequency: str
    ) -> Dict[str, Any]:
        """åŒæ­¥è‚¡ç¥¨æ‰¹æ¬¡"""
        batch_result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        for symbol in symbols:
            try:
                # è®¡ç®—åŒæ­¥èŒƒå›´
                start_date, end_date = self.calculate_sync_range(
                    symbol, target_date, frequency
                )

                if start_date is None:
                    batch_result["skipped_count"] += 1
                    continue

                # åŒæ­¥æ•°æ®
                sync_result = self.sync_symbol_range(
                    symbol, start_date, end_date, frequency
                )

                if sync_result["success_count"] > 0:
                    batch_result["success_count"] += 1
                    batch_result["sync_ranges"][symbol] = {
                        "start_date": str(start_date),
                        "end_date": str(end_date),
                        "sync_count": sync_result["success_count"],
                    }
                else:
                    batch_result["error_count"] += 1

            except Exception as e:
                logger.error(f"æ‰¹æ¬¡åŒæ­¥å¤±è´¥ {symbol}: {e}")
                batch_result["error_count"] += 1

        return batch_result

    def _get_active_symbols(self) -> List[str]:
        """è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨"""
        try:
            sql = """
            SELECT symbol FROM stocks
            WHERE status = 'active'
            ORDER BY symbol
            """
            results = self.db_manager.fetchall(sql)

            if results:
                return [row["symbol"] for row in results]
            else:
                logger.warning("æ•°æ®åº“ä¸­æ— æ´»è·ƒè‚¡ç¥¨")
                return []

        except Exception as e:
            logger.error(f"è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _is_trading_day(self, target_date: date) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥"""
        try:
            sql = """
            SELECT is_trading FROM trading_calendar
            WHERE date = ? AND market = 'CN'
            """
            result = self.db_manager.fetchone(sql, (str(target_date),))

            if result:
                return bool(result["is_trading"])
            else:
                # ä¸å†ä½¿ç”¨ç®€åŒ–fallbackï¼Œå¿…é¡»æœ‰æ­£ç¡®çš„äº¤æ˜“æ—¥å†æ•°æ®
                raise RuntimeError(f"äº¤æ˜“æ—¥å†æ•°æ®ç¼ºå¤±ï¼Œæ—¥æœŸ: {target_date}")

        except Exception as e:
            logger.error(f"æ£€æŸ¥äº¤æ˜“æ—¥å¤±è´¥: {e}")
            # ä¸å†ä½¿ç”¨ç®€åŒ–fallbackï¼Œå¿…é¡»æœ‰æ­£ç¡®çš„äº¤æ˜“æ—¥å†æ•°æ®
            raise RuntimeError(f"æ— æ³•è·å–äº¤æ˜“æ—¥å†æ•°æ®: {e}")

    def _reset_stats(self):
        """é‡ç½®åŒæ­¥ç»Ÿè®¡"""
        self.sync_stats = {
            "total_symbols": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_date_ranges": {},
            "errors": [],
        }

    def _update_sync_status(self, target_date: date, stats: Dict[str, Any]):
        """æ›´æ–°åŒæ­¥çŠ¶æ€"""
        try:
            # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼ˆä¸æ•°æ®åº“è¡¨ç»“æ„åŒ¹é…ï¼‰
            sql = """
            INSERT OR REPLACE INTO sync_status
            (symbol, frequency, last_sync_date, last_data_date, status,
             error_message, total_records, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """

            # ä¿®æ­£åŒæ­¥çŠ¶æ€åˆ¤æ–­é€»è¾‘
            effective_success = stats["success_count"] > 0
            effective_error = stats["error_count"] > 0

            if effective_success and not effective_error:
                sync_status = "completed"
            elif effective_success and effective_error:
                sync_status = "partial_success"
            else:
                sync_status = "failed"

            error_msg = f"æˆåŠŸ={stats['success_count']}, é”™è¯¯={stats['error_count']}, è·³è¿‡={stats['skipped_count']}"

            # è®¡ç®—å®é™…æ’å…¥/æ›´æ–°çš„è®°å½•æ•°
            # ä»sync_date_rangesä¸­ç»Ÿè®¡å®é™…çš„æ•°æ®è®°å½•æ•°
            actual_records_count = 0
            sync_ranges = stats.get("sync_date_ranges", {})

            for frequency_data in sync_ranges.values():
                if isinstance(frequency_data, dict) and "sync_ranges" in frequency_data:
                    for symbol_range in frequency_data["sync_ranges"].values():
                        if isinstance(symbol_range, dict):
                            actual_records_count += symbol_range.get("sync_count", 0)

            # å¦‚æœæ— æ³•ä»sync_rangesè·å–ï¼Œåˆ™æŸ¥è¯¢æ•°æ®åº“ä¸­ç›®æ ‡æ—¥æœŸçš„å®é™…è®°å½•æ•°
            if actual_records_count == 0 and sync_status in [
                "completed",
                "partial_success",
            ]:
                try:
                    actual_count_sql = """
                    SELECT COUNT(*) as count 
                    FROM market_data 
                    WHERE date = ? AND frequency = '1d'
                    """
                    count_result = self.db_manager.fetchone(
                        actual_count_sql, (str(target_date),)
                    )
                    if count_result:
                        actual_records_count = count_result["count"]
                except Exception as e:
                    logger.warning(f"æŸ¥è¯¢å®é™…è®°å½•æ•°å¤±è´¥: {e}")
                    actual_records_count = stats["success_count"]  # å¤‡ç”¨æ–¹æ¡ˆ

            # ä¸ºå¢é‡åŒæ­¥åˆ›å»ºä¸€ä¸ªæ±‡æ€»è®°å½•
            self.db_manager.execute(
                sql,
                (
                    "ALL_SYMBOLS",  # symbol
                    "1d",  # frequency
                    str(target_date),  # last_sync_date
                    str(target_date),  # last_data_date
                    sync_status,  # status
                    error_msg,  # error_message
                    actual_records_count,  # total_records - ä½¿ç”¨å®é™…è®°å½•æ•°
                    datetime.now().isoformat(),  # updated_at
                ),
            )

            logger.info(
                f"åŒæ­¥çŠ¶æ€å·²æ›´æ–°: {sync_status}, å®é™…è®°å½•æ•°: {actual_records_count}"
            )

        except Exception as e:
            logger.error(f"æ›´æ–°åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")

    def get_sync_stats(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        return self.sync_stats.copy()

    def _sync_pipeline(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """
        æµæ°´çº¿åŒæ­¥ï¼šä¸‹è½½ä¸²è¡Œï¼Œå¤„ç†å¹¶å‘

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            frequency: é¢‘ç‡
            progress_bar: è¿›åº¦æ¡

        Returns:
            åŒæ­¥ç»“æœ
        """
        result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        # è®¡ç®—éœ€è¦åŒæ­¥çš„è‚¡ç¥¨å’Œæ—¥æœŸèŒƒå›´
        sync_tasks = []
        for symbol in symbols:
            try:
                start_date, end_date = self.calculate_sync_range(
                    symbol, target_date, frequency
                )

                if start_date is None:
                    result["skipped_count"] += 1
                    if progress_bar:
                        progress_bar.update(1)
                    continue

                sync_tasks.append((symbol, start_date, end_date))

            except Exception as e:
                logger.error(f"è®¡ç®—åŒæ­¥èŒƒå›´å¤±è´¥ {symbol}: {e}")
                result["error_count"] += 1
                if progress_bar:
                    progress_bar.update(1)

        if not sync_tasks:
            return result

        # ä½¿ç”¨æ•°æ®å¤„ç†å¼•æ“çš„æµæ°´çº¿æ¨¡å¼
        # å°†åŒæ­¥ä»»åŠ¡è½¬æ¢ä¸ºå¤„ç†å¼•æ“éœ€è¦çš„æ ¼å¼
        task_symbols = [task[0] for task in sync_tasks]

        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä½¿ç”¨æœ€æ—©çš„å¼€å§‹æ—¥æœŸå’Œç›®æ ‡æ—¥æœŸ
        min_start_date = min(task[1] for task in sync_tasks)

        try:
            # è°ƒç”¨æ•°æ®å¤„ç†å¼•æ“çš„æµæ°´çº¿å¤„ç†
            pipeline_result = self.processing_engine.process_symbols_batch_pipeline(
                symbols=task_symbols,
                start_date=min_start_date,
                end_date=target_date,
                batch_size=5,  # å‡å°‘æ‰¹æ¬¡å¤§å°åˆ°5åªè‚¡ç¥¨
                max_workers=2,  # å‡å°‘å¤„ç†çº¿ç¨‹åˆ°2ä¸ª
                progress_bar=progress_bar,  # ä¼ é€’è¿›åº¦æ¡
            )

            # è½¬æ¢ç»“æœæ ¼å¼ - å¤„ç†åµŒå¥—çš„ç»Ÿä¸€é”™è¯¯å¤„ç†è¿”å›æ ¼å¼
            if isinstance(pipeline_result, dict) and pipeline_result.get(
                "success", True
            ):
                data = pipeline_result.get("data", pipeline_result)
                result["success_count"] = data.get("success_count", 0)
                result["error_count"] += data.get("error_count", 0)
                processed_symbols = data.get("processed_symbols", [])
            else:
                result["error_count"] += len(task_symbols)
                processed_symbols = []

            # ä¸ºæˆåŠŸçš„è‚¡ç¥¨åˆ›å»ºåŒæ­¥èŒƒå›´è®°å½•
            for symbol in processed_symbols:
                # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡
                for task_symbol, start_date, end_date in sync_tasks:
                    if task_symbol == symbol:
                        result["sync_ranges"][symbol] = {
                            "start_date": str(start_date),
                            "end_date": str(end_date),
                            "sync_count": 1,  # ç®€åŒ–å¤„ç†
                        }
                        break

            # è¿›åº¦æ¡å·²ç»åœ¨æµæ°´çº¿å¤„ç†ä¸­æ›´æ–°äº†ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤æ›´æ–°

        except Exception as e:
            logger.error(f"æµæ°´çº¿åŒæ­¥å¤±è´¥: {e}")
            result["error_count"] += len(sync_tasks)
            # å¦‚æœæ•´ä¸ªæµæ°´çº¿å¤±è´¥ï¼Œæ›´æ–°æ‰€æœ‰å‰©ä½™è¿›åº¦
            if progress_bar:
                progress_bar.update(len(task_symbols))

        return result

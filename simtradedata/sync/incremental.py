"""
å¢é‡åŒæ­¥å™¨

è´Ÿè´£æ£€æµ‹æœ€åæ•°æ®æ—¥æœŸï¼Œè®¡ç®—å¢é‡æ•°æ®èŒƒå›´ï¼Œæ‰§è¡Œæ‰¹é‡æ•°æ®åŒæ­¥ã€‚
"""

import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import date, datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from ..config import Config
from ..core import extract_data_safely
from ..data_sources import DataSourceManager
from ..database import DatabaseManager
from ..database.batch_writer import BatchWriter
from ..monitoring import PerformanceMonitor
from ..performance.cache_manager import CacheManager
from ..preprocessor import DataProcessingEngine

logger = logging.getLogger(__name__)


class IncrementalSync:
    """å¢é‡åŒæ­¥å™¨"""

    def __init__(
        self,
        db_manager: DatabaseManager,
        data_source_manager: DataSourceManager,
        processing_engine: DataProcessingEngine,
        config: Config = None,
    ):
        """
        åˆå§‹åŒ–å¢é‡åŒæ­¥å™¨

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            data_source_manager: æ•°æ®æºç®¡ç†å™¨
            processing_engine: æ•°æ®å¤„ç†å¼•æ“
            config: é…ç½®å¯¹è±¡
        """
        self.db_manager = db_manager
        self.data_source_manager = data_source_manager
        self.processing_engine = processing_engine
        self.config = config or Config()

        # åŒæ­¥é…ç½®
        self.max_sync_days = self.config.get("sync.max_sync_days", 30)
        self.batch_size = self.config.get("sync.batch_size", 50)
        self.max_workers = self.config.get("sync.max_workers", 3)
        self.sync_frequencies = self.config.get("sync.frequencies", ["1d"])
        self.enable_parallel = self.config.get("sync.enable_parallel", True)

        # æ™ºèƒ½è¡¥å……é…ç½®ï¼ˆé»˜è®¤ç¦ç”¨ï¼Œå¯é€šè¿‡é…ç½®å¯ç”¨ï¼‰
        self.enable_smart_backfill = self.config.get(
            "sync.enable_smart_backfill", False
        )
        self.backfill_batch_size = self.config.get("sync.backfill_batch_size", 50)
        self.backfill_sample_size = self.config.get("sync.backfill_sample_size", 10)

        # æ‰¹é‡å†™å…¥é…ç½®
        self.enable_batch_writer = self.config.get(
            "performance.batch_writer.enable", True
        )
        self.batch_write_size = self.config.get(
            "performance.batch_writer.batch_size", 100
        )

        # ç¼“å­˜é…ç½®
        self.enable_cache = self.config.get("performance.cache.enable", True)

        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = None
        if self.enable_cache:
            try:
                self.cache_manager = CacheManager(config=self.config)
                logger.info("ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ç¦ç”¨ç¼“å­˜åŠŸèƒ½")
                self.enable_cache = False

        # æ€§èƒ½ç›‘æ§é…ç½®
        self.enable_performance_monitor = self.config.get(
            "performance.monitor.enable", True
        )
        self.enable_resource_monitoring = self.config.get(
            "performance.monitor.enable_resource_monitoring", False
        )

        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        self.performance_monitor = None
        if self.enable_performance_monitor:
            try:
                self.performance_monitor = PerformanceMonitor(
                    enable_resource_monitoring=self.enable_resource_monitoring
                )
                logger.info("æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ç¦ç”¨æ€§èƒ½ç›‘æ§åŠŸèƒ½")
                self.enable_performance_monitor = False

        # åŒæ­¥ç»Ÿè®¡
        self.sync_stats = {
            "total_symbols": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_date_ranges": {},
            "errors": [],
        }

        logger.info("å¢é‡åŒæ­¥å™¨åˆå§‹åŒ–å®Œæˆ")

    def sync_all_symbols(
        self,
        target_date: date = None,
        symbols: List[str] = None,
        frequencies: List[str] = None,
        progress_bar=None,
    ) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰è‚¡ç¥¨çš„å¢é‡æ•°æ®

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ´»è·ƒè‚¡ç¥¨
            frequencies: é¢‘ç‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºé…ç½®ä¸­çš„é¢‘ç‡

        Returns:
            Dict[str, Any]: åŒæ­¥ç»“æœ
        """
        if target_date is None:
            target_date = datetime.now().date()

        if frequencies is None:
            frequencies = self.sync_frequencies

        try:
            logger.info(f"å¼€å§‹å¢é‡åŒæ­¥: ç›®æ ‡æ—¥æœŸ={target_date}, é¢‘ç‡={frequencies}")

            # ğŸ¯ å¼€å§‹æ€§èƒ½ç›‘æ§
            if self.enable_performance_monitor and self.performance_monitor:
                self.performance_monitor.start_phase("total")

            # é‡ç½®ç»Ÿè®¡
            self._reset_stats()

            # è·å–éœ€è¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                symbols = self._get_active_symbols()

            self.sync_stats["total_symbols"] = len(symbols)

            # ğŸš€ ç¼“å­˜é¢„åŠ è½½é˜¶æ®µ
            if self.enable_cache and self.cache_manager:
                try:
                    logger.info("å¼€å§‹é¢„åŠ è½½ç¼“å­˜...")

                    # é¢„åŠ è½½äº¤æ˜“æ—¥å†ï¼ˆæœ€è¿‘2å¹´ï¼‰
                    calendar_start = target_date - timedelta(days=730)  # 2å¹´
                    calendar_result = self.cache_manager.load_trading_calendar(
                        self.db_manager, calendar_start, target_date, market="CN"
                    )

                    # å¤„ç† unified_error_handler åŒ…è£…çš„è¿”å›å€¼
                    if isinstance(calendar_result, dict) and "data" in calendar_result:
                        calendar_count = calendar_result["data"]
                    else:
                        calendar_count = calendar_result

                    logger.info(f"é¢„åŠ è½½äº¤æ˜“æ—¥å†: {calendar_count} å¤©")

                    # é¢„åŠ è½½æ´»è·ƒè‚¡ç¥¨å…ƒæ•°æ®
                    if symbols:
                        metadata_result = self.cache_manager.load_stock_metadata_batch(
                            self.db_manager, symbols
                        )

                        # å¤„ç† unified_error_handler åŒ…è£…çš„è¿”å›å€¼
                        if (
                            isinstance(metadata_result, dict)
                            and "data" in metadata_result
                        ):
                            metadata_count = metadata_result["data"]
                        else:
                            metadata_count = metadata_result

                        logger.info(f"é¢„åŠ è½½è‚¡ç¥¨å…ƒæ•°æ®: {metadata_count} åªè‚¡ç¥¨")

                except Exception as cache_error:
                    logger.warning(
                        f"ç¼“å­˜é¢„åŠ è½½å¤±è´¥: {cache_error}ï¼Œå°†ç»§ç»­ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢"
                    )

            # ğŸ”™ å†å²å›å¡«é˜¶æ®µï¼šæ£€æŸ¥å¹¶è¡¥å……å†å²æ•°æ®ç¼ºå£
            # å†å²å›å¡«é…ç½®ï¼šå¤ç”¨æ™ºèƒ½è¡¥å……çš„é…ç½®å‚æ•°
            enable_historical_backfill = self.config.get(
                "sync.enable_historical_backfill", True
            )
            historical_backfill_sample_size = self.config.get(
                "sync.historical_backfill_sample_size", self.backfill_sample_size
            )
            historical_backfill_batch_size = self.config.get(
                "sync.historical_backfill_batch_size", self.backfill_batch_size
            )

            historical_backfill_stats = {
                "enabled": enable_historical_backfill,
                "checked_symbols": 0,
                "needs_backfill_symbols": 0,
                "backfilled_symbols": 0,
                "backfilled_records": 0,
                "backfill_errors": 0,
            }

            if enable_historical_backfill:
                logger.info("å¼€å§‹å†å²æ•°æ®ç¼ºå£æ£€æµ‹...")

                # ğŸ¯ å¼€å§‹å†å²å›å¡«é˜¶æ®µç›‘æ§
                if self.enable_performance_monitor and self.performance_monitor:
                    self.performance_monitor.start_phase("historical_backfill")

                # æ£€æŸ¥å‰å‡ åªè‚¡ç¥¨æ¥ä¼°ç®—æ•´ä½“æƒ…å†µ
                sample_size = min(historical_backfill_sample_size, len(symbols))
                sample_symbols = symbols[:sample_size]
                needs_backfill_count = 0

                for symbol in sample_symbols:
                    historical_backfill_stats["checked_symbols"] += 1
                    gap = self.detect_historical_gap(symbol, frequencies[0])
                    if gap:
                        needs_backfill_count += 1

                # å¦‚æœæ ·æœ¬ä¸­æœ‰éœ€è¦å›å¡«çš„æ•°æ®ï¼Œåˆ™å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œå†å²å›å¡«
                if needs_backfill_count > 0:
                    backfill_ratio = needs_backfill_count / sample_size
                    estimated_total = int(len(symbols) * backfill_ratio)
                    logger.info(
                        f"æ£€æµ‹åˆ°å†å²æ•°æ®ç¼ºå£ï¼šæ ·æœ¬ä¸­ {needs_backfill_count}/{sample_size} åªè‚¡ç¥¨éœ€è¦å›å¡«"
                    )
                    logger.info(
                        f"é¢„ä¼°å…¨éƒ¨ {len(symbols)} åªè‚¡ç¥¨ä¸­çº¦ {estimated_total} åªéœ€è¦å›å¡«ï¼Œå¼€å§‹å†å²å›å¡«..."
                    )

                    # å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œå†å²å›å¡«ï¼ˆåˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜é—®é¢˜ï¼‰
                    batch_size = historical_backfill_batch_size

                    for i in range(0, len(symbols), batch_size):
                        batch_symbols = symbols[i : i + batch_size]
                        batch_num = i // batch_size + 1
                        total_batches = (len(symbols) + batch_size - 1) // batch_size

                        logger.info(
                            f"å†å²å›å¡«æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç† {len(batch_symbols)} åªè‚¡ç¥¨"
                        )

                        for symbol in batch_symbols:
                            try:
                                historical_backfill_stats["checked_symbols"] += 1

                                # æ£€æŸ¥æ˜¯å¦æœ‰å†å²ç¼ºå£
                                gap = self.detect_historical_gap(symbol, frequencies[0])

                                if gap:
                                    historical_backfill_stats[
                                        "needs_backfill_symbols"
                                    ] += 1
                                    gap_start, gap_end = gap

                                    # æ‰§è¡Œå†å²å›å¡«
                                    backfill_result = self.sync_symbol_range(
                                        symbol, gap_start, gap_end, frequencies[0]
                                    )

                                    if backfill_result.get("success_count", 0) > 0:
                                        historical_backfill_stats[
                                            "backfilled_symbols"
                                        ] += 1
                                        historical_backfill_stats[
                                            "backfilled_records"
                                        ] += backfill_result.get("success_count", 0)
                                        logger.info(
                                            f"å†å²å›å¡«æˆåŠŸ: {symbol} {gap_start} åˆ° {gap_end}, "
                                            f"å›å¡« {backfill_result.get('success_count', 0)} æ¡è®°å½•"
                                        )
                                    else:
                                        historical_backfill_stats[
                                            "backfill_errors"
                                        ] += 1
                                        logger.warning(
                                            f"å†å²å›å¡«å¤±è´¥: {symbol} {gap_start} åˆ° {gap_end}"
                                        )

                            except Exception as e:
                                logger.warning(f"å†å²å›å¡«è‚¡ç¥¨ {symbol} æ—¶å‡ºé”™: {e}")
                                historical_backfill_stats["backfill_errors"] += 1

                    logger.info(
                        f"å†å²å›å¡«å®Œæˆ: æ£€æŸ¥äº† {historical_backfill_stats['checked_symbols']} åªè‚¡ç¥¨ï¼Œ"
                        f"å›å¡«äº† {historical_backfill_stats['backfilled_symbols']} åªè‚¡ç¥¨çš„ "
                        f"{historical_backfill_stats['backfilled_records']} æ¡å†å²è®°å½•"
                    )

                    # ğŸ¯ ç»“æŸå†å²å›å¡«é˜¶æ®µç›‘æ§
                    if self.enable_performance_monitor and self.performance_monitor:
                        self.performance_monitor.end_phase(
                            "historical_backfill",
                            historical_backfill_stats["backfilled_records"],
                        )

                else:
                    logger.info("æ ·æœ¬æ£€æŸ¥æ˜¾ç¤ºå†å²æ•°æ®å®Œæ•´ï¼Œè·³è¿‡å†å²å›å¡«é˜¶æ®µ")

            # ğŸš€ æ™ºèƒ½è¡¥å……é˜¶æ®µï¼šæ£€æŸ¥å¹¶è¡¥å……å†å²æ•°æ®çš„è¡ç”Ÿå­—æ®µ
            backfill_stats = {
                "enabled": self.enable_smart_backfill,
                "checked_symbols": 0,
                "needs_backfill_symbols": 0,
                "backfilled_symbols": 0,
                "backfilled_records": 0,
                "backfill_errors": 0,
            }

            if self.enable_smart_backfill:
                logger.info("å¼€å§‹æ™ºèƒ½æ•°æ®è´¨é‡æ£€æŸ¥å’Œè¡¥å……...")

                # ğŸ¯ å¼€å§‹æ™ºèƒ½è¡¥å……é˜¶æ®µç›‘æ§
                if self.enable_performance_monitor and self.performance_monitor:
                    self.performance_monitor.start_phase("smart_backfill")

                # æ£€æŸ¥å‰å‡ åªè‚¡ç¥¨æ¥ä¼°ç®—æ•´ä½“æƒ…å†µ
                sample_size = min(self.backfill_sample_size, len(symbols))
                sample_symbols = symbols[:sample_size]
                needs_backfill_count = 0

                for symbol in sample_symbols:
                    quality_check = self.check_data_quality(symbol, frequencies[0])
                    if quality_check.get("needs_backfill", False):
                        needs_backfill_count += 1

                # å¦‚æœæ ·æœ¬ä¸­æœ‰éœ€è¦è¡¥å……çš„æ•°æ®ï¼Œåˆ™å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œæ™ºèƒ½è¡¥å……
                if needs_backfill_count > 0:
                    backfill_ratio = needs_backfill_count / sample_size
                    estimated_total = int(len(symbols) * backfill_ratio)
                    logger.info(
                        f"æ£€æµ‹åˆ°æ•°æ®è´¨é‡é—®é¢˜ï¼šæ ·æœ¬ä¸­ {needs_backfill_count}/{sample_size} åªè‚¡ç¥¨éœ€è¦è¡¥å……è¡ç”Ÿå­—æ®µ"
                    )
                    logger.info(
                        f"é¢„ä¼°å…¨éƒ¨ {len(symbols)} åªè‚¡ç¥¨ä¸­çº¦ {estimated_total} åªéœ€è¦è¡¥å……ï¼Œå¼€å§‹æ™ºèƒ½è¡¥å……..."
                    )

                    # å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œæ™ºèƒ½è¡¥å……ï¼ˆåˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜é—®é¢˜ï¼‰
                    batch_size = self.backfill_batch_size

                    for i in range(0, len(symbols), batch_size):
                        batch_symbols = symbols[i : i + batch_size]
                        batch_num = i // batch_size + 1
                        total_batches = (len(symbols) + batch_size - 1) // batch_size

                        logger.info(
                            f"æ™ºèƒ½è¡¥å……æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç† {len(batch_symbols)} åªè‚¡ç¥¨"
                        )

                        for symbol in batch_symbols:
                            try:
                                # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å……
                                quality_check = self.check_data_quality(
                                    symbol, frequencies[0]
                                )
                                backfill_stats["checked_symbols"] += 1

                                if quality_check.get("needs_backfill", False):
                                    backfill_stats["needs_backfill_symbols"] += 1

                                    # æ‰§è¡Œæ™ºèƒ½è¡¥å……
                                    backfill_result = self.smart_backfill_symbol(
                                        symbol, frequencies[0]
                                    )

                                    if backfill_result.get("success", False):
                                        backfill_stats["backfilled_symbols"] += 1
                                        backfill_stats[
                                            "backfilled_records"
                                        ] += backfill_result.get("updated_count", 0)
                                    else:
                                        backfill_stats["backfill_errors"] += 1

                            except Exception as e:
                                logger.warning(f"æ™ºèƒ½è¡¥å……è‚¡ç¥¨ {symbol} æ—¶å‡ºé”™: {e}")
                                backfill_stats["backfill_errors"] += 1

                        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ›´æ–°ä¸»è¿›åº¦æ¡ï¼Œå› ä¸ºæ­£å¸¸å¢é‡åŒæ­¥é˜¶æ®µä¼šæ›´æ–°
                        # é¿å…é‡å¤æ›´æ–°å¯¼è‡´è¿›åº¦è¶…è¿‡100%

                    logger.info(
                        f"æ™ºèƒ½è¡¥å……å®Œæˆ: æ£€æŸ¥äº† {backfill_stats['checked_symbols']} åªè‚¡ç¥¨ï¼Œ"
                        f"è¡¥å……äº† {backfill_stats['backfilled_symbols']} åªè‚¡ç¥¨çš„ {backfill_stats['backfilled_records']} æ¡è®°å½•"
                    )

                    # ğŸ¯ ç»“æŸæ™ºèƒ½è¡¥å……é˜¶æ®µç›‘æ§
                    if self.enable_performance_monitor and self.performance_monitor:
                        self.performance_monitor.end_phase(
                            "smart_backfill", backfill_stats["backfilled_records"]
                        )

                else:
                    logger.info("æ ·æœ¬æ£€æŸ¥æ˜¾ç¤ºæ•°æ®è´¨é‡è‰¯å¥½ï¼Œè·³è¿‡æ™ºèƒ½è¡¥å……é˜¶æ®µ")
            else:
                logger.info("æ™ºèƒ½è¡¥å……åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ•°æ®è´¨é‡æ£€æŸ¥")

            # ğŸ“ˆ æ­£å¸¸å¢é‡åŒæ­¥é˜¶æ®µ

            # ğŸ¯ å¼€å§‹å¢é‡åŒæ­¥é˜¶æ®µç›‘æ§
            if self.enable_performance_monitor and self.performance_monitor:
                self.performance_monitor.start_phase("incremental_sync")

            # æŒ‰é¢‘ç‡åŒæ­¥
            for frequency in frequencies:
                freq_result = self._sync_frequency_data(
                    symbols, target_date, frequency, progress_bar
                )
                self.sync_stats["sync_date_ranges"][frequency] = freq_result

            # ğŸ¯ ç»“æŸå¢é‡åŒæ­¥é˜¶æ®µç›‘æ§
            if self.enable_performance_monitor and self.performance_monitor:
                self.performance_monitor.end_phase(
                    "incremental_sync", self.sync_stats["success_count"]
                )

            # æ›´æ–°åŒæ­¥çŠ¶æ€
            self._update_sync_status(target_date, self.sync_stats)

            # å°†å†å²å›å¡«ç»Ÿè®¡ä¿¡æ¯æ·»åŠ åˆ°ç»“æœä¸­
            self.sync_stats["historical_backfill"] = historical_backfill_stats

            # å°†æ™ºèƒ½è¡¥å……ç»Ÿè®¡ä¿¡æ¯æ·»åŠ åˆ°ç»“æœä¸­
            self.sync_stats["smart_backfill"] = backfill_stats

            # ä¿®æ­£æˆåŠŸç‡è®¡ç®—ï¼šå¦‚æœæœ‰æˆåŠŸçš„è‚¡ç¥¨å¤„ç†ï¼Œå³ä½¿æœ‰éƒ¨åˆ†é”™è¯¯ä¹Ÿåº”å½“æ•´ä½“æ ‡è®°ä¸ºéƒ¨åˆ†æˆåŠŸ
            effective_success = self.sync_stats["success_count"] > 0
            effective_error = self.sync_stats["error_count"] > 0

            if effective_success and not effective_error:
                result_status = "completed"
            elif effective_success and effective_error:
                result_status = "partial_success"
            else:
                result_status = "failed"

            logger.info(
                f"å¢é‡åŒæ­¥å®Œæˆ: æˆåŠŸ={self.sync_stats['success_count']}, "
                f"é”™è¯¯={self.sync_stats['error_count']}, "
                f"è·³è¿‡={self.sync_stats['skipped_count']}, "
                f"æ•´ä½“çŠ¶æ€={result_status}"
            )

            if historical_backfill_stats["backfilled_symbols"] > 0:
                logger.info(
                    f"å†å²å›å¡«å®Œæˆ: å›å¡«äº† {historical_backfill_stats['backfilled_symbols']} åªè‚¡ç¥¨çš„ "
                    f"{historical_backfill_stats['backfilled_records']} æ¡å†å²è®°å½•"
                )

            if backfill_stats["backfilled_symbols"] > 0:
                logger.info(
                    f"æ™ºèƒ½è¡¥å……å®Œæˆ: è¡¥å……äº† {backfill_stats['backfilled_symbols']} åªè‚¡ç¥¨çš„ "
                    f"{backfill_stats['backfilled_records']} æ¡å†å²è®°å½•çš„è¡ç”Ÿå­—æ®µ"
                )

            # ğŸ¯ ç»“æŸæ€»ä½“ç›‘æ§å¹¶ç”ŸæˆæŠ¥å‘Š
            if self.enable_performance_monitor and self.performance_monitor:
                self.performance_monitor.end_phase(
                    "total", self.sync_stats["success_count"]
                )

                # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
                try:
                    report = self.performance_monitor.generate_report()

                    # è®°å½•æ–‡æœ¬æ ¼å¼æŠ¥å‘Šåˆ°æ—¥å¿—
                    logger.info("\n" + report.to_text())

                    # è¯†åˆ«ç“¶é¢ˆå¹¶è®°å½•ä¼˜åŒ–å»ºè®®
                    bottlenecks = report.bottlenecks
                    if bottlenecks:
                        logger.warning("âš ï¸  æ£€æµ‹åˆ°æ€§èƒ½ç“¶é¢ˆ:")
                        for bottleneck in bottlenecks:
                            logger.warning(f"  - {bottleneck}")
                        logger.info(
                            "ğŸ’¡ ä¼˜åŒ–å»ºè®®: è€ƒè™‘è°ƒæ•´ç›¸å…³é…ç½®å‚æ•°ä»¥æå‡ç“¶é¢ˆé˜¶æ®µçš„æ€§èƒ½"
                        )

                    # å°†æŠ¥å‘Šæ·»åŠ åˆ°åŒæ­¥ç»Ÿè®¡ä¸­
                    self.sync_stats["performance_report"] = report.to_dict()

                except Exception as report_error:
                    logger.warning(f"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {report_error}")

            return self.sync_stats.copy()

        except Exception as e:
            logger.error(f"å¢é‡åŒæ­¥å¤±è´¥: {e}")
            raise

    def check_data_quality(self, symbol: str, frequency: str = "1d") -> Dict[str, Any]:
        """
        æ£€æŸ¥è‚¡ç¥¨æ•°æ®è´¨é‡ï¼Œç‰¹åˆ«æ˜¯è¡ç”Ÿå­—æ®µçš„å®Œæ•´æ€§

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Dict[str, Any]: æ•°æ®è´¨é‡æŠ¥å‘Š
        """
        try:
            sql = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(CASE WHEN change_percent IS NULL THEN 1 END) as null_change_percent,
                COUNT(CASE WHEN prev_close IS NULL THEN 1 END) as null_prev_close,
                COUNT(CASE WHEN amplitude IS NULL THEN 1 END) as null_amplitude,
                COUNT(CASE WHEN source LIKE '%enhanced' THEN 1 END) as enhanced_records,
                MIN(date) as earliest_date,
                MAX(date) as latest_date
            FROM market_data 
            WHERE symbol = ? AND frequency = ?
            """

            result = self.db_manager.fetchone(sql, (symbol, frequency))

            if result:
                total = result["total_records"]
                null_derived = result["null_change_percent"]

                return {
                    "symbol": symbol,
                    "total_records": total,
                    "null_change_percent": null_derived,
                    "null_prev_close": result["null_prev_close"],
                    "null_amplitude": result["null_amplitude"],
                    "enhanced_records": result["enhanced_records"],
                    "earliest_date": result["earliest_date"],
                    "latest_date": result["latest_date"],
                    "needs_backfill": null_derived > 0,
                    "backfill_ratio": null_derived / total if total > 0 else 0,
                }
            else:
                return {
                    "symbol": symbol,
                    "total_records": 0,
                    "needs_backfill": False,
                    "backfill_ratio": 0,
                }

        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®è´¨é‡å¤±è´¥ {symbol}: {e}")
            return {
                "symbol": symbol,
                "total_records": 0,
                "needs_backfill": False,
                "error": str(e),
            }

    def smart_backfill_symbol(
        self, symbol: str, frequency: str = "1d"
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½è¡¥å……å•ä¸ªè‚¡ç¥¨çš„è¡ç”Ÿå­—æ®µæ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Dict[str, Any]: è¡¥å……ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½è¡¥å……è‚¡ç¥¨æ•°æ®: {symbol}")

            # è·å–è¯¥è‚¡ç¥¨çš„æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ—¥æœŸæ’åº
            data_sql = """
            SELECT date, open, high, low, close, volume, amount
            FROM market_data 
            WHERE symbol = ? AND frequency = ?
            ORDER BY date
            """

            records = self.db_manager.fetchall(data_sql, (symbol, frequency))

            if not records:
                return {
                    "symbol": symbol,
                    "success": False,
                    "updated_count": 0,
                    "message": "æ— æ•°æ®è®°å½•",
                }

            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œæ‰¹é‡è®¡ç®—
            import numpy as np
            import pandas as pd

            df = pd.DataFrame([dict(record) for record in records])
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date")

            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloatç±»å‹
            numeric_columns = ["open", "high", "low", "close", "volume", "amount"]
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

            # è®¡ç®—å‰ä¸€æ—¥æ”¶ç›˜ä»·
            df["prev_close_new"] = df["close"].shift(1)

            # è®¡ç®—æ¶¨è·Œé¢
            df["change_amount_new"] = df["close"] - df["prev_close_new"]

            # è®¡ç®—æ¶¨è·Œå¹…ï¼ˆç™¾åˆ†æ¯”ï¼‰
            df["change_percent_new"] = np.where(
                df["prev_close_new"] > 0,
                (df["change_amount_new"] / df["prev_close_new"] * 100).round(4),
                0.0,
            )

            # è®¡ç®—æŒ¯å¹…
            df["amplitude_new"] = np.where(
                df["prev_close_new"] > 0,
                ((df["high"] - df["low"]) / df["prev_close_new"] * 100).round(4),
                0.0,
            )

            # è®¡ç®—æ¶¨è·Œåœä»·æ ¼
            df["high_limit_new"] = np.where(
                df["prev_close_new"] > 0, (df["prev_close_new"] * 1.1).round(2), None
            )
            df["low_limit_new"] = np.where(
                df["prev_close_new"] > 0, (df["prev_close_new"] * 0.9).round(2), None
            )

            # åˆ¤æ–­æ¶¨åœè·Œåœ
            df["is_limit_up_new"] = False
            df["is_limit_down_new"] = False

            valid_high_limit = df["high_limit_new"].notna()
            valid_low_limit = df["low_limit_new"].notna()

            if valid_high_limit.any():
                df.loc[valid_high_limit, "is_limit_up_new"] = (
                    df.loc[valid_high_limit, "close"]
                    >= df.loc[valid_high_limit, "high_limit_new"]
                )
            if valid_low_limit.any():
                df.loc[valid_low_limit, "is_limit_down_new"] = (
                    df.loc[valid_low_limit, "close"]
                    <= df.loc[valid_low_limit, "low_limit_new"]
                )

            # ç¬¬ä¸€è¡Œè®¾ä¸ºé»˜è®¤å€¼
            if len(df) > 0:
                first_idx = df.index[0]
                df.loc[
                    first_idx,
                    [
                        "prev_close_new",
                        "change_amount_new",
                        "change_percent_new",
                        "amplitude_new",
                        "high_limit_new",
                        "low_limit_new",
                        "is_limit_up_new",
                        "is_limit_down_new",
                    ],
                ] = [None, 0.0, 0.0, 0.0, None, None, False, False]

            # æ‰¹é‡æ›´æ–°æ•°æ®åº“
            updated_count = 0

            # å°è¯•ä½¿ç”¨æ‰¹é‡å†™å…¥ä¼˜åŒ–
            if self.enable_batch_writer:
                try:
                    logger.debug(
                        f"ä½¿ç”¨ BatchWriter æ‰¹é‡æ›´æ–° {symbol} çš„ {len(df)} æ¡è®°å½•"
                    )

                    # åˆå§‹åŒ– BatchWriter
                    batch_writer = BatchWriter(
                        self.db_manager,
                        batch_size=self.batch_write_size,
                        auto_flush=False,  # æ‰‹åŠ¨æ§åˆ¶åˆ·æ–°
                    )

                    # å‡†å¤‡æ‰¹é‡æ›´æ–°çš„ SQL
                    # æ³¨æ„: SQLite ä¸æ”¯æŒ UPDATE çš„ executemanyï¼Œéœ€è¦ä½¿ç”¨ INSERT OR REPLACE
                    # é¦–å…ˆè·å–å®Œæ•´è®°å½•ï¼Œç„¶åç”¨ INSERT OR REPLACE æ›´æ–°
                    for _, row in df.iterrows():
                        try:
                            # æ„å»ºæ›´æ–°è®°å½•ï¼ˆåªåŒ…å«éœ€è¦æ›´æ–°çš„å­—æ®µï¼‰
                            update_record = {
                                "symbol": symbol,
                                "date": row["date"].strftime("%Y-%m-%d"),
                                "frequency": frequency,
                                "prev_close": (
                                    row["prev_close_new"]
                                    if pd.notna(row["prev_close_new"])
                                    else None
                                ),
                                "change_amount": (
                                    row["change_amount_new"]
                                    if pd.notna(row["change_amount_new"])
                                    else 0.0
                                ),
                                "change_percent": (
                                    row["change_percent_new"]
                                    if pd.notna(row["change_percent_new"])
                                    else 0.0
                                ),
                                "amplitude": (
                                    row["amplitude_new"]
                                    if pd.notna(row["amplitude_new"])
                                    else 0.0
                                ),
                                "high_limit": (
                                    row["high_limit_new"]
                                    if pd.notna(row["high_limit_new"])
                                    else None
                                ),
                                "low_limit": (
                                    row["low_limit_new"]
                                    if pd.notna(row["low_limit_new"])
                                    else None
                                ),
                                "is_limit_up": bool(row["is_limit_up_new"]),
                                "is_limit_down": bool(row["is_limit_down_new"]),
                            }

                            # ä½¿ç”¨ä¸“ç”¨çš„ UPDATE SQL æ‰§è¡Œæ‰¹é‡æ›´æ–°
                            batch_writer.add_record(
                                "_update_market_data", update_record
                            )

                        except Exception as e:
                            logger.warning(
                                f"å‡†å¤‡æ‰¹é‡æ›´æ–°è®°å½•å¤±è´¥ {symbol} {row['date']}: {e}"
                            )

                    # æ‰‹åŠ¨åˆ·æ–°ï¼šä½¿ç”¨è‡ªå®šä¹‰ UPDATE SQL
                    if batch_writer.get_buffer_size("_update_market_data") > 0:
                        update_sql = """
                        UPDATE market_data
                        SET prev_close = ?, change_amount = ?, change_percent = ?, amplitude = ?,
                            high_limit = ?, low_limit = ?, is_limit_up = ?, is_limit_down = ?,
                            source = CASE WHEN source LIKE '%enhanced' THEN source ELSE 'smart_backfilled_enhanced' END,
                            quality_score = 100
                        WHERE symbol = ? AND date = ? AND frequency = ?
                        """

                        records = batch_writer._buffer["_update_market_data"]
                        params_list = [
                            (
                                rec["prev_close"],
                                rec["change_amount"],
                                rec["change_percent"],
                                rec["amplitude"],
                                rec["high_limit"],
                                rec["low_limit"],
                                rec["is_limit_up"],
                                rec["is_limit_down"],
                                rec["symbol"],
                                rec["date"],
                                rec["frequency"],
                            )
                            for rec in records
                        ]

                        # ä½¿ç”¨ execute_batch æ‰§è¡Œæ‰¹é‡ UPDATE
                        updated_count = batch_writer.execute_batch(
                            update_sql, params_list, use_transaction=True
                        )

                        logger.info(
                            f"BatchWriter æ‰¹é‡æ›´æ–°å®Œæˆ {symbol}: {updated_count} æ¡è®°å½•"
                        )

                        # è·å–æ‰¹é‡å†™å…¥ç»Ÿè®¡
                        batch_stats = batch_writer.get_stats()
                        logger.debug(
                            f"æ‰¹é‡å†™å…¥ç»Ÿè®¡: æ€»è®°å½•={batch_stats['total_records']}, "
                            f"æ‰¹æ¬¡æ•°={batch_stats['total_batches']}, "
                            f"å¹³å‡æ‰¹æ¬¡å¤§å°={batch_stats['avg_batch_size']:.1f}, "
                            f"å¹³å‡åˆ·æ–°æ—¶é—´={batch_stats['avg_flush_time']:.2f}ms"
                        )

                except Exception as batch_error:
                    logger.warning(
                        f"BatchWriter æ‰¹é‡æ›´æ–°å¤±è´¥ {symbol}: {batch_error}, é™çº§åˆ°é€æ¡æ›´æ–°"
                    )
                    # é™çº§åˆ°é€æ¡æ›´æ–°
                    self._fallback_update_records(df, symbol, frequency, updated_count)
                    updated_count = self._count_updated_records(df, symbol, frequency)

            else:
                # æ‰¹é‡å†™å…¥æœªå¯ç”¨ï¼Œä½¿ç”¨é€æ¡æ›´æ–°
                logger.debug(f"BatchWriter æœªå¯ç”¨ï¼Œä½¿ç”¨é€æ¡æ›´æ–° {symbol}")
                updated_count = self._fallback_update_records(
                    df, symbol, frequency, updated_count
                )

            logger.info(f"æ™ºèƒ½è¡¥å……å®Œæˆ {symbol}: æ›´æ–° {updated_count} æ¡è®°å½•")

            return {
                "symbol": symbol,
                "success": True,
                "updated_count": updated_count,
                "total_records": len(df),
                "message": f"æˆåŠŸæ›´æ–° {updated_count} æ¡è®°å½•",
            }

        except Exception as e:
            logger.error(f"æ™ºèƒ½è¡¥å……å¤±è´¥ {symbol}: {e}")
            return {
                "symbol": symbol,
                "success": False,
                "updated_count": 0,
                "error": str(e),
            }

    def _fallback_update_records(
        self, df, symbol: str, frequency: str, initial_count: int = 0
    ) -> int:
        """
        é™çº§é€æ¡æ›´æ–°è®°å½•

        Args:
            df: DataFrame åŒ…å«æ›´æ–°æ•°æ®
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡
            initial_count: åˆå§‹è®¡æ•°

        Returns:
            int: æ›´æ–°çš„è®°å½•æ•°
        """
        updated_count = initial_count
        update_sql = """
        UPDATE market_data
        SET prev_close = ?, change_amount = ?, change_percent = ?, amplitude = ?,
            high_limit = ?, low_limit = ?, is_limit_up = ?, is_limit_down = ?,
            source = CASE WHEN source LIKE '%enhanced' THEN source ELSE 'smart_backfilled_enhanced' END,
            quality_score = 100
        WHERE symbol = ? AND date = ? AND frequency = ?
        """

        import pandas as pd

        for _, row in df.iterrows():
            try:
                params = (
                    row["prev_close_new"] if pd.notna(row["prev_close_new"]) else None,
                    (
                        row["change_amount_new"]
                        if pd.notna(row["change_amount_new"])
                        else 0.0
                    ),
                    (
                        row["change_percent_new"]
                        if pd.notna(row["change_percent_new"])
                        else 0.0
                    ),
                    row["amplitude_new"] if pd.notna(row["amplitude_new"]) else 0.0,
                    row["high_limit_new"] if pd.notna(row["high_limit_new"]) else None,
                    row["low_limit_new"] if pd.notna(row["low_limit_new"]) else None,
                    bool(row["is_limit_up_new"]),
                    bool(row["is_limit_down_new"]),
                    symbol,
                    row["date"].strftime("%Y-%m-%d"),
                    frequency,
                )

                self.db_manager.execute(update_sql, params)
                updated_count += 1

            except Exception as e:
                logger.warning(f"æ›´æ–°è®°å½•å¤±è´¥ {symbol} {row['date']}: {e}")

        return updated_count

    def _count_updated_records(self, df, symbol: str, frequency: str) -> int:
        """
        ç»Ÿè®¡å®é™…æ›´æ–°çš„è®°å½•æ•°

        Args:
            df: DataFrame
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            int: æ›´æ–°çš„è®°å½•æ•°
        """
        try:
            # æŸ¥è¯¢æ›´æ–°åçš„è®°å½•æ•°ï¼ˆsourceåŒ…å«'enhanced'ï¼‰
            sql = """
            SELECT COUNT(*) as count
            FROM market_data
            WHERE symbol = ? AND frequency = ?
              AND source LIKE '%enhanced'
            """
            result = self.db_manager.fetchone(sql, (symbol, frequency))
            return result["count"] if result else len(df)
        except Exception:
            return len(df)

    def sync_symbol_range(
        self, symbol: str, start_date: date, end_date: date, frequency: str = "1d"
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å•ä¸ªè‚¡ç¥¨çš„æ—¥æœŸèŒƒå›´æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            frequency: é¢‘ç‡

        Returns:
            Dict[str, Any]: åŒæ­¥ç»“æœ
        """
        try:
            logger.info(
                f"åŒæ­¥è‚¡ç¥¨èŒƒå›´æ•°æ®: {symbol} {start_date} åˆ° {end_date} {frequency}"
            )

            result = {
                "symbol": symbol,
                "start_date": str(start_date),
                "end_date": str(end_date),
                "frequency": frequency,
                "success_count": 0,
                "error_count": 0,
                "sync_dates": [],
            }

            # æ‰¹é‡åŒæ­¥æ•´ä¸ªæ—¥æœŸèŒƒå›´
            try:
                # ä½¿ç”¨æ•°æ®å¤„ç†å¼•æ“æ‰¹é‡å¤„ç†æ—¥æœŸèŒƒå›´æ•°æ®
                process_result = self.processing_engine.process_stock_data(
                    symbol, start_date, end_date, frequency, force_update=True
                )

                # ç»Ÿä¸€æ•°æ®æ ¼å¼å¤„ç† - é¿å…å¤šæ¬¡æ‹†åŒ…
                actual_result = extract_data_safely(process_result)

                # ç»Ÿè®¡ç»“æœ
                result["success_count"] = len(actual_result.get("processed_dates", []))
                result["error_count"] = len(actual_result.get("failed_dates", []))
                result["sync_dates"] = actual_result.get("processed_dates", [])

                logger.debug(
                    f"æ‰¹é‡å¤„ç†ç»“æœ: æˆåŠŸ={result['success_count']}, å¤±è´¥={result['error_count']}"
                )

            except Exception as e:
                logger.error(f"æ‰¹é‡åŒæ­¥å¤±è´¥ {symbol} {start_date}-{end_date}: {e}")
                result["error_count"] = 1

            logger.info(
                f"è‚¡ç¥¨èŒƒå›´åŒæ­¥å®Œæˆ: {symbol}, æˆåŠŸ={result['success_count']}, "
                f"é”™è¯¯={result['error_count']}"
            )

            # æ›´æ–°ç¼“å­˜ï¼šå¦‚æœåŒæ­¥æˆåŠŸï¼Œæ›´æ–°æœ€åæ•°æ®æ—¥æœŸç¼“å­˜
            if result["success_count"] > 0 and self.enable_cache and self.cache_manager:
                try:
                    self.cache_manager.set_last_data_date(symbol, frequency, end_date)
                    logger.debug(f"å·²æ›´æ–°ç¼“å­˜: {symbol} æœ€åæ•°æ®æ—¥æœŸ={end_date}")
                except Exception as cache_error:
                    logger.warning(f"æ›´æ–°ç¼“å­˜å¤±è´¥: {cache_error}")

            return result

        except Exception as e:
            logger.error(f"è‚¡ç¥¨èŒƒå›´åŒæ­¥å¤±è´¥ {symbol}: {e}")
            raise

    def get_last_data_date(self, symbol: str, frequency: str = "1d") -> Optional[date]:
        """
        è·å–è‚¡ç¥¨çš„æœ€åæ•°æ®æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Optional[date]: æœ€åæ•°æ®æ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰æ•°æ®åˆ™è¿”å›None
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
            if self.enable_cache and self.cache_manager:
                cached_date = self.cache_manager.get_last_data_date(symbol, frequency)

                # å¤„ç† unified_error_handler åŒ…è£…çš„è¿”å›å€¼
                if isinstance(cached_date, dict) and "data" in cached_date:
                    cached_date = cached_date["data"]

                if cached_date is not None:
                    return cached_date

            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
            sql = """
            SELECT MAX(date) as last_date
            FROM market_data
            WHERE symbol = ? AND frequency = ?
            """

            result = self.db_manager.fetchone(sql, (symbol, frequency))

            if result and result["last_date"]:
                last_date = datetime.strptime(result["last_date"], "%Y-%m-%d").date()

                # æ›´æ–°ç¼“å­˜
                if self.enable_cache and self.cache_manager:
                    self.cache_manager.set_last_data_date(symbol, frequency, last_date)

                return last_date
            else:
                return None

        except Exception as e:
            logger.error(f"è·å–æœ€åæ•°æ®æ—¥æœŸå¤±è´¥ {symbol}: {e}")
            return None

    def get_earliest_data_date(
        self, symbol: str, frequency: str = "1d"
    ) -> Optional[date]:
        """
        è·å–è‚¡ç¥¨çš„æœ€æ—©æ•°æ®æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Optional[date]: æœ€æ—©æ•°æ®æ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰æ•°æ®åˆ™è¿”å›None
        """
        try:
            sql = """
            SELECT MIN(date) as earliest_date
            FROM market_data
            WHERE symbol = ? AND frequency = ?
            """

            result = self.db_manager.fetchone(sql, (symbol, frequency))

            if result and result["earliest_date"]:
                earliest_date = datetime.strptime(
                    result["earliest_date"], "%Y-%m-%d"
                ).date()
                return earliest_date
            else:
                return None

        except Exception as e:
            logger.error(f"è·å–æœ€æ—©æ•°æ®æ—¥æœŸå¤±è´¥ {symbol}: {e}")
            return None

    def detect_historical_gap(
        self, symbol: str, frequency: str = "1d"
    ) -> Optional[Tuple[date, date]]:
        """
        æ£€æµ‹å†å²æ•°æ®ç¼ºå£

        æ£€æŸ¥æ•°æ®åº“ä¸­æœ€æ—©çš„æ•°æ®æ—¥æœŸæ˜¯å¦æ™šäºé…ç½®çš„é»˜è®¤èµ·å§‹æ—¥æœŸï¼Œ
        å¦‚æœæ˜¯ï¼Œåˆ™è¿”å›éœ€è¦å›å¡«çš„æ—¥æœŸèŒƒå›´ã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: é¢‘ç‡

        Returns:
            Optional[Tuple[date, date]]: å¦‚æœæœ‰å†å²ç¼ºå£ï¼Œè¿”å›(default_start, æœ€æ—©äº¤æ˜“æ—¥å‰ä¸€æ—¥)ï¼›
                                         å¦åˆ™è¿”å›None
        """
        try:
            # è·å–é…ç½®çš„é»˜è®¤èµ·å§‹æ—¥æœŸ
            default_start_str = self.config.get("sync.default_start_date", "2020-01-01")
            default_start = datetime.strptime(default_start_str, "%Y-%m-%d").date()

            # è·å–æ•°æ®åº“ä¸­æœ€æ—©çš„æ•°æ®æ—¥æœŸ
            earliest_date = self.get_earliest_data_date(symbol, frequency)

            if earliest_date is None:
                # æ²¡æœ‰æ•°æ®ï¼Œä¸æ˜¯å†å²ç¼ºå£é—®é¢˜
                return None

            # æ£€æŸ¥æœ€æ—©æ—¥æœŸæ˜¯å¦æ™šäºé»˜è®¤èµ·å§‹æ—¥æœŸ
            if earliest_date > default_start:
                # æŸ¥æ‰¾é…ç½®èµ·å§‹æ—¥æœŸä¹‹åã€æœ€æ—©æ•°æ®æ—¥æœŸä¹‹å‰çš„äº¤æ˜“æ—¥
                sql = """
                SELECT date FROM trading_calendar
                WHERE date >= ? AND date < ? AND market = 'CN' AND is_trading = 1
                ORDER BY date DESC
                LIMIT 1
                """
                result = self.db_manager.fetchone(
                    sql, (str(default_start), str(earliest_date))
                )

                if result:
                    # æ‰¾åˆ°äº†äº¤æ˜“æ—¥ï¼Œè¿™æ˜¯çœŸå®çš„å†å²ç¼ºå£
                    gap_end = datetime.strptime(result["date"], "%Y-%m-%d").date()
                    logger.debug(
                        f"æ£€æµ‹åˆ°å†å²ç¼ºå£ {symbol}: {default_start} åˆ° {gap_end} "
                        f"(å½“å‰æœ€æ—©æ•°æ®: {earliest_date})"
                    )
                    return (default_start, gap_end)
                else:
                    # æ²¡æœ‰æ‰¾åˆ°äº¤æ˜“æ—¥ï¼Œè¯´æ˜ default_start åˆ° earliest_date ä¹‹é—´æ²¡æœ‰äº¤æ˜“æ—¥
                    # è¿™ä¸æ˜¯çœŸæ­£çš„ç¼ºå£ï¼Œæ•°æ®å·²ç»ä»ç¬¬ä¸€ä¸ªäº¤æ˜“æ—¥å¼€å§‹äº†
                    logger.debug(
                        f"é…ç½®èµ·å§‹æ—¥æœŸ {default_start} åˆ°æœ€æ—©æ•°æ®æ—¥æœŸ {earliest_date} ä¹‹é—´æ²¡æœ‰äº¤æ˜“æ—¥ï¼Œæ— éœ€å›å¡«"
                    )
                    return None
            else:
                # æ²¡æœ‰å†å²ç¼ºå£
                return None

        except Exception as e:
            logger.error(f"æ£€æµ‹å†å²ç¼ºå£å¤±è´¥ {symbol}: {e}")
            return None

    def calculate_sync_range(
        self,
        symbol: str,
        target_date: date,
        frequency: str = "1d",
        check_historical_gap: bool = False,
    ) -> Tuple[Optional[date], date]:
        """
        è®¡ç®—å¢é‡åŒæ­¥çš„æ—¥æœŸèŒƒå›´

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            target_date: ç›®æ ‡æ—¥æœŸ
            frequency: é¢‘ç‡
            check_historical_gap: æ˜¯å¦æ£€æŸ¥å¹¶ä¼˜å…ˆå¤„ç†å†å²ç¼ºå£ï¼ˆé»˜è®¤Falseï¼Œä¿æŒå‘åå…¼å®¹ï¼‰

        Returns:
            Tuple[Optional[date], date]: (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ)
        """
        try:
            # è·å–æœ€åæ•°æ®æ—¥æœŸ
            last_date = self.get_last_data_date(symbol, frequency)

            if last_date is None:
                # æ²¡æœ‰å†å²æ•°æ®ï¼Œä»é…ç½®çš„é»˜è®¤å¼€å§‹æ—¥æœŸåŒæ­¥
                default_start = self.config.get("sync.default_start_date", "2020-01-01")
                start_date = datetime.strptime(default_start, "%Y-%m-%d").date()

                # é™åˆ¶æœ€å¤§åŒæ­¥å¤©æ•°
                max_start = target_date - timedelta(days=self.max_sync_days)
                if start_date < max_start:
                    start_date = max_start

                logger.info(f"é¦–æ¬¡åŒæ­¥ {symbol}: {start_date} åˆ° {target_date}")
                return start_date, target_date
            else:
                # ğŸ†• æ–°å¢ï¼šæ£€æŸ¥å†å²ç¼ºå£ï¼ˆä»…åœ¨æ˜¾å¼å¼€å¯æ—¶ï¼‰
                if check_historical_gap:
                    historical_gap = self.detect_historical_gap(symbol, frequency)
                    if historical_gap:
                        gap_start, gap_end = historical_gap
                        logger.info(
                            f"å†å²å›å¡« {symbol}: {gap_start} åˆ° {gap_end} "
                            f"(å½“å‰æœ€æ—©æ•°æ®: {self.get_earliest_data_date(symbol, frequency)})"
                        )
                        return gap_start, gap_end

                # æœ‰å†å²æ•°æ®ï¼Œä»æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹
                start_date = last_date + timedelta(days=1)

                # æ£€æŸ¥æ˜¯å¦å°è¯•åŒæ­¥æœªæ¥æ—¥æœŸ
                today = datetime.now().date()
                if target_date > today:
                    target_date = today
                    logger.debug(f"ç›®æ ‡æ—¥æœŸè°ƒæ•´ä¸ºä»Šå¤©: {target_date}")

                if start_date > target_date:
                    # å·²ç»æ˜¯æœ€æ–°æ•°æ®
                    logger.debug(f"æ•°æ®å·²æ˜¯æœ€æ–° {symbol}: æœ€åæ—¥æœŸ={last_date}")
                    return None, target_date

                logger.debug(f"å¢é‡åŒæ­¥ {symbol}: {start_date} åˆ° {target_date}")
                return start_date, target_date

        except Exception as e:
            logger.error(f"è®¡ç®—åŒæ­¥èŒƒå›´å¤±è´¥ {symbol}: {e}")
            return None, target_date

    def _sync_frequency_data(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """åŒæ­¥ç‰¹å®šé¢‘ç‡çš„æ•°æ®"""
        logger.info(f"åŒæ­¥é¢‘ç‡æ•°æ®: {frequency}, è‚¡ç¥¨æ•°é‡: {len(symbols)}")

        result = {
            "frequency": frequency,
            "total_symbols": len(symbols),
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        # ä½¿ç”¨æµæ°´çº¿æ¨¡å¼ï¼šä¸‹è½½ä¸²è¡Œï¼Œå¤„ç†å¹¶å‘
        result.update(
            self._sync_pipeline(symbols, target_date, frequency, progress_bar)
        )

        # æ›´æ–°æ€»ç»Ÿè®¡
        self.sync_stats["success_count"] += result["success_count"]
        self.sync_stats["error_count"] += result["error_count"]
        self.sync_stats["skipped_count"] += result["skipped_count"]

        return result

    def _sync_sequential(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """ä¸²è¡ŒåŒæ­¥"""
        result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        for symbol in symbols:
            try:
                # è®¡ç®—åŒæ­¥èŒƒå›´
                start_date, end_date = self.calculate_sync_range(
                    symbol, target_date, frequency
                )

                if start_date is None:
                    result["skipped_count"] += 1
                    # æ›´æ–°è¿›åº¦æ¡
                    if progress_bar:
                        progress_bar.update(1)
                    continue

                # åŒæ­¥æ•°æ®
                sync_result = self.sync_symbol_range(
                    symbol, start_date, end_date, frequency
                )

                if sync_result["success_count"] > 0:
                    result["success_count"] += 1
                    result["sync_ranges"][symbol] = {
                        "start_date": str(start_date),
                        "end_date": str(end_date),
                        "sync_count": sync_result["success_count"],
                    }
                else:
                    result["error_count"] += 1

                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    progress_bar.update(1)

            except Exception as e:
                logger.error(f"ä¸²è¡ŒåŒæ­¥å¤±è´¥ {symbol}: {e}")
                result["error_count"] += 1
                self.sync_stats["errors"].append(
                    {"symbol": symbol, "frequency": frequency, "error": str(e)}
                )
                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    progress_bar.update(1)

        return result

    def _sync_parallel(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """å¹¶è¡ŒåŒæ­¥"""
        result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        # åˆ†æ‰¹å¤„ç†
        symbol_batches = [
            symbols[i : i + self.batch_size]
            for i in range(0, len(symbols), self.batch_size)
        ]

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_batch = {
                executor.submit(
                    self._sync_symbol_batch, batch, target_date, frequency
                ): batch
                for batch in symbol_batches
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    batch_result = future.result()
                    result["success_count"] += batch_result["success_count"]
                    result["error_count"] += batch_result["error_count"]
                    result["skipped_count"] += batch_result["skipped_count"]
                    result["sync_ranges"].update(batch_result["sync_ranges"])

                except Exception as e:
                    logger.error(f"å¹¶è¡ŒåŒæ­¥æ‰¹æ¬¡å¤±è´¥: {batch}, é”™è¯¯: {e}")
                    result["error_count"] += len(batch)

        return result

    def _sync_symbol_batch(
        self, symbols: List[str], target_date: date, frequency: str
    ) -> Dict[str, Any]:
        """åŒæ­¥è‚¡ç¥¨æ‰¹æ¬¡"""
        batch_result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        for symbol in symbols:
            try:
                # è®¡ç®—åŒæ­¥èŒƒå›´
                start_date, end_date = self.calculate_sync_range(
                    symbol, target_date, frequency
                )

                if start_date is None:
                    batch_result["skipped_count"] += 1
                    continue

                # åŒæ­¥æ•°æ®
                sync_result = self.sync_symbol_range(
                    symbol, start_date, end_date, frequency
                )

                if sync_result["success_count"] > 0:
                    batch_result["success_count"] += 1
                    batch_result["sync_ranges"][symbol] = {
                        "start_date": str(start_date),
                        "end_date": str(end_date),
                        "sync_count": sync_result["success_count"],
                    }
                else:
                    batch_result["error_count"] += 1

            except Exception as e:
                logger.error(f"æ‰¹æ¬¡åŒæ­¥å¤±è´¥ {symbol}: {e}")
                batch_result["error_count"] += 1

        return batch_result

    def _get_active_symbols(self) -> List[str]:
        """è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨"""
        try:
            sql = """
            SELECT symbol FROM stocks
            WHERE status = 'active'
            ORDER BY symbol
            """
            results = self.db_manager.fetchall(sql)

            if results:
                return [row["symbol"] for row in results]
            else:
                logger.warning("æ•°æ®åº“ä¸­æ— æ´»è·ƒè‚¡ç¥¨")
                return []

        except Exception as e:
            logger.error(f"è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _is_trading_day(self, target_date: date) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥"""
        try:
            # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
            if self.enable_cache and self.cache_manager:
                cached_result = self.cache_manager.is_trading_day(
                    target_date, market="CN"
                )

                # å¤„ç† unified_error_handler åŒ…è£…çš„è¿”å›å€¼
                if isinstance(cached_result, dict) and "data" in cached_result:
                    cached_result = cached_result["data"]

                if cached_result is not None:
                    return cached_result

            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
            sql = """
            SELECT is_trading FROM trading_calendar
            WHERE date = ? AND market = 'CN'
            """
            result = self.db_manager.fetchone(sql, (str(target_date),))

            if result:
                return bool(result["is_trading"])
            else:
                # ä¸å†ä½¿ç”¨ç®€åŒ–fallbackï¼Œå¿…é¡»æœ‰æ­£ç¡®çš„äº¤æ˜“æ—¥å†æ•°æ®
                raise RuntimeError(f"äº¤æ˜“æ—¥å†æ•°æ®ç¼ºå¤±ï¼Œæ—¥æœŸ: {target_date}")

        except Exception as e:
            logger.error(f"æ£€æŸ¥äº¤æ˜“æ—¥å¤±è´¥: {e}")
            # ä¸å†ä½¿ç”¨ç®€åŒ–fallbackï¼Œå¿…é¡»æœ‰æ­£ç¡®çš„äº¤æ˜“æ—¥å†æ•°æ®
            raise RuntimeError(f"æ— æ³•è·å–äº¤æ˜“æ—¥å†æ•°æ®: {e}")

    def _reset_stats(self):
        """é‡ç½®åŒæ­¥ç»Ÿè®¡"""
        self.sync_stats = {
            "total_symbols": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_date_ranges": {},
            "errors": [],
        }

    def _update_sync_status(self, target_date: date, stats: Dict[str, Any]):
        """æ›´æ–°åŒæ­¥çŠ¶æ€"""
        try:
            # ç¡®å®šåŒæ­¥çŠ¶æ€
            effective_success = stats["success_count"] > 0
            effective_error = stats["error_count"] > 0

            if effective_success and not effective_error:
                sync_status = "completed"
            elif effective_success and effective_error:
                sync_status = "partial_success"
            else:
                sync_status = "failed"

            # æ„å»ºé”™è¯¯æ¶ˆæ¯
            error_msg = f"æˆåŠŸ={stats['success_count']}, é”™è¯¯={stats['error_count']}, è·³è¿‡={stats['skipped_count']}"

            # ä½¿ç”¨æˆåŠŸæ•°é‡ä½œä¸ºè®°å½•æ•°ï¼ˆç®€åŒ–ç»Ÿè®¡ï¼‰
            actual_records_count = stats["success_count"]

            # æ’å…¥æ±‡æ€»è®°å½•
            sql = """
            INSERT OR REPLACE INTO sync_status
            (symbol, frequency, last_sync_date, last_data_date, status,
             error_message, total_records, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """

            self.db_manager.execute(
                sql,
                (
                    "ALL_SYMBOLS",  # symbol
                    "1d",  # frequency
                    str(target_date),  # last_sync_date
                    str(target_date),  # last_data_date
                    sync_status,  # status
                    error_msg,  # error_message
                    actual_records_count,  # total_records
                    datetime.now().isoformat(),  # updated_at
                ),
            )

            logger.info(
                f"åŒæ­¥çŠ¶æ€å·²æ›´æ–°: {sync_status}, è®°å½•æ•°: {actual_records_count}"
            )

        except Exception as e:
            logger.error(f"æ›´æ–°åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")

    def get_sync_stats(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        return self.sync_stats.copy()

    def _sync_pipeline(
        self, symbols: List[str], target_date: date, frequency: str, progress_bar=None
    ) -> Dict[str, Any]:
        """
        æµæ°´çº¿åŒæ­¥ï¼šä¸‹è½½ä¸²è¡Œï¼Œå¤„ç†å¹¶å‘

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ
            frequency: é¢‘ç‡
            progress_bar: è¿›åº¦æ¡

        Returns:
            åŒæ­¥ç»“æœ
        """
        result = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "sync_ranges": {},
        }

        # è®¡ç®—éœ€è¦åŒæ­¥çš„è‚¡ç¥¨å’Œæ—¥æœŸèŒƒå›´
        sync_tasks = []
        for symbol in symbols:
            try:
                start_date, end_date = self.calculate_sync_range(
                    symbol, target_date, frequency
                )

                if start_date is None:
                    result["skipped_count"] += 1
                    if progress_bar:
                        progress_bar.update(1)
                    continue

                sync_tasks.append((symbol, start_date, end_date))

            except Exception as e:
                logger.error(f"è®¡ç®—åŒæ­¥èŒƒå›´å¤±è´¥ {symbol}: {e}")
                result["error_count"] += 1
                if progress_bar:
                    progress_bar.update(1)

        if not sync_tasks:
            return result

        # ä½¿ç”¨æ•°æ®å¤„ç†å¼•æ“çš„æµæ°´çº¿æ¨¡å¼
        # å°†åŒæ­¥ä»»åŠ¡è½¬æ¢ä¸ºå¤„ç†å¼•æ“éœ€è¦çš„æ ¼å¼
        task_symbols = [task[0] for task in sync_tasks]

        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä½¿ç”¨æœ€æ—©çš„å¼€å§‹æ—¥æœŸå’Œç›®æ ‡æ—¥æœŸ
        min_start_date = min(task[1] for task in sync_tasks)

        try:
            # è°ƒç”¨æ•°æ®å¤„ç†å¼•æ“çš„æµæ°´çº¿å¤„ç†
            pipeline_result = self.processing_engine.process_symbols_batch_pipeline(
                symbols=task_symbols,
                start_date=min_start_date,
                end_date=target_date,
                batch_size=5,  # å‡å°‘æ‰¹æ¬¡å¤§å°åˆ°5åªè‚¡ç¥¨
                max_workers=2,  # å‡å°‘å¤„ç†çº¿ç¨‹åˆ°2ä¸ª
                progress_bar=progress_bar,  # ä¼ é€’è¿›åº¦æ¡
            )

            # è½¬æ¢ç»“æœæ ¼å¼ - å¤„ç†åµŒå¥—çš„ç»Ÿä¸€é”™è¯¯å¤„ç†è¿”å›æ ¼å¼
            if isinstance(pipeline_result, dict) and pipeline_result.get(
                "success", True
            ):
                data = pipeline_result.get("data", pipeline_result)
                result["success_count"] = data.get("success_count", 0)
                result["error_count"] += data.get("error_count", 0)
                processed_symbols = data.get("processed_symbols", [])
            else:
                result["error_count"] += len(task_symbols)
                processed_symbols = []

            # ä¸ºæˆåŠŸçš„è‚¡ç¥¨åˆ›å»ºåŒæ­¥èŒƒå›´è®°å½•
            for symbol in processed_symbols:
                # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡
                for task_symbol, start_date, end_date in sync_tasks:
                    if task_symbol == symbol:
                        result["sync_ranges"][symbol] = {
                            "start_date": str(start_date),
                            "end_date": str(end_date),
                            "sync_count": 1,  # ç®€åŒ–å¤„ç†
                        }
                        break

            # è¿›åº¦æ¡å·²ç»åœ¨æµæ°´çº¿å¤„ç†ä¸­æ›´æ–°äº†ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤æ›´æ–°

        except Exception as e:
            logger.error(f"æµæ°´çº¿åŒæ­¥å¤±è´¥: {e}")
            result["error_count"] += len(sync_tasks)
            # å¦‚æœæ•´ä¸ªæµæ°´çº¿å¤±è´¥ï¼Œæ›´æ–°æ‰€æœ‰å‰©ä½™è¿›åº¦
            if progress_bar:
                progress_bar.update(len(task_symbols))

        return result

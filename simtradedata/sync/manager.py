"""
åŒæ­¥ç®¡ç†å™¨

ç»Ÿä¸€ç®¡ç†å¢é‡åŒæ­¥ã€ç¼ºå£æ£€æµ‹å’Œæ•°æ®éªŒè¯åŠŸèƒ½ã€‚
"""

# æ ‡å‡†åº“å¯¼å…¥
import logging
import re
import uuid
from datetime import date, datetime, timedelta
from typing import Any, Dict, List, Optional

# é¡¹ç›®å†…å¯¼å…¥
from ..config import Config
from ..core import BaseManager, ValidationError, unified_error_handler
from ..data_sources import DataSourceManager
from ..database import DatabaseManager
from ..preprocessor import DataProcessingEngine
from ..utils.progress_bar import (
    create_phase_progress,
    log_error,
    log_phase_complete,
    log_phase_start,
    update_phase_description,
)
from .gap_detector import GapDetector
from .incremental import IncrementalSync
from .validator import DataValidator

logger = logging.getLogger(__name__)


# å¸¸é‡å®šä¹‰
class SyncConstants:
    """åŒæ­¥ç›¸å…³å¸¸é‡"""

    # æ•°æ®éªŒè¯èŒƒå›´
    MIN_REPORT_YEAR = 1990
    MAX_PE_RATIO = 1000
    MAX_PB_RATIO = 100

    # æ•°å­—å•ä½è½¬æ¢
    WAN_MULTIPLIER = 10000
    YI_MULTIPLIER = 100000000

    # æ—¥æœŸæ ¼å¼
    DATE_FORMAT = "%Y-%m-%d"

    # é‡è¯•æ¬¡æ•°
    DEFAULT_MAX_RETRIES = 3


class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""

    @staticmethod
    def is_valid_financial_data(data: Dict[str, Any]) -> bool:
        """éªŒè¯è´¢åŠ¡æ•°æ®æœ‰æ•ˆæ€§"""
        if not data or not isinstance(data, dict):
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è´¢åŠ¡æŒ‡æ ‡
        revenue = data.get("revenue", 0)
        net_profit = data.get("net_profit", 0)
        total_assets = data.get("total_assets", 0)

        # è‡³å°‘è¦æœ‰ä¸€ä¸ªéé›¶çš„ä¸»è¦è´¢åŠ¡æŒ‡æ ‡
        return (
            (revenue and revenue > 0)
            or (total_assets and total_assets > 0)
            or (net_profit != 0)  # å‡€åˆ©æ¶¦å¯ä»¥ä¸ºè´Ÿ
        )

    @staticmethod
    def is_valid_valuation_data(data: Dict[str, Any]) -> bool:
        """éªŒè¯ä¼°å€¼æ•°æ®æœ‰æ•ˆæ€§"""
        if not data or not isinstance(data, dict):
            return False

        pe_ratio = data.get("pe_ratio", 0)
        pb_ratio = data.get("pb_ratio", 0)

        # PE/PBåº”è¯¥ä¸ºæ­£æ•°ä¸”åœ¨åˆç†èŒƒå›´å†…
        # ç§»é™¤å¯¹market_capçš„ä¾èµ–ï¼Œå› ä¸ºå¸‚å€¼ç°åœ¨æ˜¯è®¡ç®—å€¼è€Œéå­˜å‚¨å€¼
        return (pe_ratio and 0 < pe_ratio < SyncConstants.MAX_PE_RATIO) or (
            pb_ratio and 0 < pb_ratio < SyncConstants.MAX_PB_RATIO
        )

    @staticmethod
    def is_valid_report_date(report_date: str, symbol: Optional[str] = None) -> bool:
        """éªŒè¯æŠ¥å‘ŠæœŸæœ‰æ•ˆæ€§"""
        try:
            report_dt = datetime.strptime(report_date, SyncConstants.DATE_FORMAT)
            current_dt = datetime.now()

            # æŠ¥å‘ŠæœŸä¸èƒ½æ˜¯æœªæ¥æ—¥æœŸ
            if report_dt > current_dt:
                return False

            # æŠ¥å‘ŠæœŸä¸èƒ½å¤ªä¹…è¿œ
            if report_dt.year < SyncConstants.MIN_REPORT_YEAR:
                return False

            return True
        except (ValueError, TypeError):
            return False

    @staticmethod
    def is_valid_stock_basic_info(data: Dict[str, Any]) -> bool:
        """éªŒè¯è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æœ‰æ•ˆæ€§"""
        if not data or not isinstance(data, dict):
            return False

        # æ£€æŸ¥å…³é”®å­—æ®µ
        symbol = data.get("symbol", "")
        name = data.get("name", "")
        market = data.get("market", "")

        return bool(symbol and name and market)


class SyncManager(BaseManager):
    """åŒæ­¥ç®¡ç†å™¨"""

    # ç±»å‹æ³¨è§£å±æ€§ï¼ˆç”±BaseManageråŠ¨æ€æ³¨å…¥ï¼‰
    db_manager: DatabaseManager
    data_source_manager: DataSourceManager
    processing_engine: DataProcessingEngine

    def __init__(
        self,
        db_manager: DatabaseManager,
        data_source_manager: DataSourceManager,
        processing_engine: DataProcessingEngine,
        config: Optional[Config] = None,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            data_source_manager: æ•°æ®æºç®¡ç†å™¨
            processing_engine: æ•°æ®å¤„ç†å¼•æ“
            config: é…ç½®å¯¹è±¡
        """
        # åˆå§‹åŒ–ç¼“å­˜
        self._market_cache = {}
        self._stock_info_cache = {}

        super().__init__(
            config=config,
            db_manager=db_manager,
            data_source_manager=data_source_manager,
            processing_engine=processing_engine,
            **kwargs,
        )

    def _init_specific_config(self):
        """åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨ç‰¹å®šé…ç½®"""
        # åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨ç‰¹å®šé…ç½®
        self.enable_auto_gap_fix = self._get_config("auto_gap_fix", True)
        self.enable_validation = self._get_config("enable_validation", True)
        self.max_gap_fix_days = self._get_config("max_gap_fix_days", 7)

        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.batch_size = self._get_config("batch_size", 100)
        self.enable_cache = self._get_config("enable_cache", True)
        self.cache_ttl = self._get_config("cache_ttl", 3600)  # 1å°æ—¶

    def _init_components(self):
        """åˆå§‹åŒ–å­ç»„ä»¶"""
        # åˆå§‹åŒ–å­ç»„ä»¶
        self.incremental_sync = IncrementalSync(
            self.db_manager,
            self.data_source_manager,
            self.processing_engine,
            self.config,
        )
        self.gap_detector = GapDetector(self.db_manager, self.config)
        self.validator = DataValidator(self.db_manager, self.config)

    def _get_required_attributes(self) -> List[str]:
        """å¿…éœ€å±æ€§åˆ—è¡¨"""
        return [
            "db_manager",
            "data_source_manager",
            "processing_engine",
            "incremental_sync",
            "gap_detector",
            "validator",
        ]

    def _extract_data_safely(self, data: Any) -> Any:
        """
        ç»Ÿä¸€çš„æ•°æ®æ ¼å¼å¤„ç†æ–¹æ³•ï¼Œé¿å…å¤šæ¬¡æ‹†åŒ…

        Args:
            data: å¯èƒ½è¢«åŒ…è£…çš„æ•°æ®

        Returns:
            Any: æ‹†åŒ…åçš„å®é™…æ•°æ®
        """
        if not data:
            return None

        # å¦‚æœæ˜¯æ ‡å‡†æˆåŠŸå“åº”æ ¼å¼ {"success": True, "data": ..., "count": ...}
        if isinstance(data, dict) and "success" in data:
            if data.get("success"):
                return data.get("data")
            else:
                # å¤±è´¥å“åº”ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›None
                error_msg = data.get("error", "æœªçŸ¥é”™è¯¯")
                self.logger.warning(f"æ•°æ®æºè¿”å›å¤±è´¥: {error_msg}")
                return None

        # å¦‚æœæ˜¯ç®€å•åŒ…è£…æ ¼å¼ {"data": ...} (æ²¡æœ‰successå­—æ®µ)
        elif isinstance(data, dict) and "data" in data and "success" not in data:
            return data["data"]

        # å¦åˆ™ç›´æ¥è¿”å›åŸæ•°æ®
        return data

    @unified_error_handler(return_dict=True)
    def run_full_sync(
        self,
        target_date: Optional[date] = None,
        symbols: Optional[List[str]] = None,
        frequencies: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åŒæ­¥æµç¨‹

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ´»è·ƒè‚¡ç¥¨
            frequencies: é¢‘ç‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºé…ç½®ä¸­çš„é¢‘ç‡

        Returns:
            Dict[str, Any]: å®Œæ•´åŒæ­¥ç»“æœ
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé¢‘ç‡ï¼Œä½¿ç”¨é»˜è®¤é¢‘ç‡
        if frequencies is None:
            frequencies = ["1d"]

        # å¦‚æœæ²¡æœ‰æŒ‡å®šsymbolsï¼Œä½¿ç”¨é»˜è®¤å€¼
        if symbols is None:
            symbols = []

        if not target_date:
            raise ValidationError("ç›®æ ‡æ—¥æœŸä¸èƒ½ä¸ºç©º")

        if target_date is None:
            target_date = datetime.now().date()

        # é™åˆ¶ç›®æ ‡æ—¥æœŸä¸èƒ½è¶…è¿‡ä»Šå¤©ï¼Œä½¿ç”¨åˆç†çš„å†å²æ—¥æœŸ
        today = datetime.now().date()
        if target_date > today:
            # å¦‚æœç›®æ ‡æ—¥æœŸæ˜¯æœªæ¥ï¼Œä½¿ç”¨æœ€è¿‘çš„äº¤æ˜“æ—¥
            target_date = date(2025, 1, 24)  # ä½¿ç”¨å·²çŸ¥æœ‰æ•°æ®çš„æ—¥æœŸ
            self._log_warning("run_full_sync", f"ç›®æ ‡æ—¥æœŸè°ƒæ•´ä¸ºå†å²æ—¥æœŸ: {target_date}")

        try:
            self._log_method_start("run_full_sync", target_date=target_date)
            start_time = datetime.now()

            full_result = {
                "target_date": str(target_date),
                "start_time": start_time.isoformat(),
                "phases": {},
                "summary": {
                    "total_phases": 0,
                    "successful_phases": 0,
                    "failed_phases": 0,
                },
            }

            # ğŸ”„ æå‰è¿›è¡Œæ–­ç‚¹ç»­ä¼ æ£€æŸ¥ï¼ˆåœ¨åŸºç¡€æ•°æ®æ›´æ–°ä¹‹å‰ï¼‰
            # å…ˆè·å–è‚¡ç¥¨åˆ—è¡¨ç”¨äºæ–­ç‚¹ç»­ä¼ æ£€æŸ¥
            if symbols is None:
                symbols = []
            if not symbols:
                symbols = self._get_active_stocks_from_db()
                if not symbols:
                    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰è‚¡ç¥¨ï¼Œä¸èƒ½è¿›è¡Œæ–­ç‚¹ç»­ä¼ ï¼Œæ‰§è¡Œå®Œæ•´æµç¨‹
                    self.logger.info("æ•°æ®åº“ä¸­æ²¡æœ‰è‚¡ç¥¨ï¼Œæ— æ³•è¿›è¡Œæ–­ç‚¹ç»­ä¼ æ£€æŸ¥")
                else:
                    self.logger.info(f"è·å–åˆ°{len(symbols)}åªæ´»è·ƒè‚¡ç¥¨ç”¨äºæ–­ç‚¹ç»­ä¼ æ£€æŸ¥")

            # å¦‚æœæœ‰è‚¡ç¥¨åˆ—è¡¨ï¼Œæ£€æŸ¥æ–­ç‚¹ç»­ä¼ æ¡ä»¶
            if symbols:
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å·²å®Œæˆçš„æ‰©å±•æ•°æ®è®°å½•
                result = self.db_manager.fetchone(
                    "SELECT COUNT(*) as count FROM extended_sync_status WHERE target_date = ? AND status = 'completed'",
                    (str(target_date),),
                )
                completed_count = result["count"] if result else 0

                if completed_count > 0:  # å¦‚æœæœ‰å·²å®Œæˆè®°å½•ï¼Œæ‰§è¡Œæ–­ç‚¹ç»­ä¼ 
                    self.logger.info(
                        f"ğŸ”„ æ£€æµ‹åˆ°æ–­ç‚¹ç»­ä¼ : å‘ç°{completed_count}ä¸ªå·²å®Œæˆè®°å½•"
                    )

                    # é‡æ–°è®¡ç®—éœ€è¦å¤„ç†çš„è‚¡ç¥¨ï¼ˆåŸºäºæ­£ç¡®çš„symbolsåˆ—è¡¨ï¼‰
                    extended_symbols_to_process = (
                        self._get_extended_data_symbols_to_process(symbols, target_date)
                    )

                    # å¦‚æœæ‰€æœ‰æ‰©å±•æ•°æ®éƒ½å·²å®Œæˆï¼Œç›´æ¥è·³è¿‡æ‰€æœ‰é˜¶æ®µ
                    if len(extended_symbols_to_process) == 0:
                        self.logger.info("ğŸ‰ æ£€æµ‹åˆ°æ‰€æœ‰æ•°æ®å·²å®Œæˆï¼Œè·³è¿‡æ•´ä¸ªåŒæ­¥æµç¨‹")
                        full_result["phases"]["all_completed"] = {
                            "status": "completed",
                            "message": "æ‰€æœ‰æ•°æ®å·²å®Œæˆ",
                        }
                        full_result["summary"][
                            "successful_phases"
                        ] = 4  # å‡è®¾4ä¸ªé˜¶æ®µéƒ½å®Œæˆ
                        return full_result

                    # è®¡ç®—å®Œæˆè¿›åº¦
                    total_stocks = len(symbols)
                    remaining_stocks = len(extended_symbols_to_process)
                    completion_rate = (
                        (total_stocks - remaining_stocks) / total_stocks
                        if total_stocks > 0
                        else 0
                    )

                    self.logger.info(
                        f"ğŸ“Š æ–­ç‚¹ç»­ä¼ çŠ¶æ€: æ€»è®¡{total_stocks}åªï¼Œå·²å®Œæˆ{completion_rate:.1%}ï¼Œå‰©ä½™{remaining_stocks}åª"
                    )

                    # ç›´æ¥è·³åˆ°æ‰©å±•æ•°æ®åŒæ­¥é˜¶æ®µ
                    self.logger.info(
                        "â­ï¸ è·³è¿‡åŸºç¡€æ•°æ®æ›´æ–°å’Œå¢é‡åŒæ­¥ï¼Œç›´æ¥è¿›å…¥æ‰©å±•æ•°æ®åŒæ­¥"
                    )
                    full_result["phases"]["calendar_update"] = {
                        "status": "skipped",
                        "message": "æ–­ç‚¹ç»­ä¼ è·³è¿‡",
                    }
                    full_result["phases"]["stock_list_update"] = {
                        "status": "skipped",
                        "message": "æ–­ç‚¹ç»­ä¼ è·³è¿‡",
                    }
                    full_result["phases"]["incremental_sync"] = {
                        "status": "skipped",
                        "message": "æ–­ç‚¹ç»­ä¼ è·³è¿‡",
                    }
                    full_result["summary"][
                        "successful_phases"
                    ] += 3  # æ ‡è®°è·³è¿‡çš„é˜¶æ®µä¸ºæˆåŠŸ

                    # é˜¶æ®µ2: åŒæ­¥æ‰©å±•æ•°æ®ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                    log_phase_start("é˜¶æ®µ2", "æ‰©å±•æ•°æ®åŒæ­¥ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰")

                    with create_phase_progress(
                        "phase2",
                        len(extended_symbols_to_process),
                        "æ‰©å±•æ•°æ®åŒæ­¥",
                        "è‚¡ç¥¨",
                    ) as pbar:
                        try:
                            extended_result = self._sync_extended_data(
                                extended_symbols_to_process, target_date, pbar
                            )
                            full_result["phases"]["extended_data_sync"] = {
                                "status": "completed",
                                "result": extended_result,
                            }
                            full_result["summary"]["successful_phases"] += 1

                            log_phase_complete(
                                "æ‰©å±•æ•°æ®åŒæ­¥",
                                {
                                    "è´¢åŠ¡æ•°æ®": f"{extended_result.get('financials_count', 0)}æ¡",
                                    "ä¼°å€¼æ•°æ®": f"{extended_result.get('valuations_count', 0)}æ¡",
                                    "å¤„ç†è‚¡ç¥¨": f"{extended_result.get('processed_symbols', 0)}åª",
                                },
                            )

                            # å®Œæˆæ—¶é—´
                            end_time = datetime.now()
                            full_result["end_time"] = end_time.isoformat()
                            full_result["duration_seconds"] = (
                                end_time - start_time
                            ).total_seconds()
                            full_result["summary"]["total_phases"] = 4

                            return full_result

                        except Exception as e:
                            log_error(f"æ‰©å±•æ•°æ®åŒæ­¥å¤±è´¥: {e}")
                            full_result["phases"]["extended_data_sync"] = {
                                "status": "failed",
                                "error": str(e),
                            }
                            full_result["summary"]["failed_phases"] += 1
                            return full_result
                else:
                    self.logger.info("ğŸ†• æœªæ£€æµ‹åˆ°æ‰©å±•æ•°æ®è®°å½•ï¼Œæ‰§è¡Œå®Œæ•´åŒæ­¥æµç¨‹")

            # å¦‚æœæ˜¯å…¨æ–°åŒæ­¥æˆ–å®Œæˆåº¦å¾ˆä½ï¼Œæ‰§è¡Œå®Œæ•´æµç¨‹
            self.logger.info("ğŸš€ æ‰§è¡Œå®Œæ•´åŒæ­¥æµç¨‹")

            # é˜¶æ®µ0: æ›´æ–°åŸºç¡€æ•°æ®ï¼ˆäº¤æ˜“æ—¥å†å’Œè‚¡ç¥¨åˆ—è¡¨ï¼‰
            log_phase_start("é˜¶æ®µ0", "æ›´æ–°åŸºç¡€æ•°æ®")

            with create_phase_progress("phase0", 2, "åŸºç¡€æ•°æ®æ›´æ–°", "é¡¹") as pbar:
                try:
                    # æ›´æ–°äº¤æ˜“æ—¥å†
                    update_phase_description("æ›´æ–°äº¤æ˜“æ—¥å†")
                    calendar_result = self._update_trading_calendar(target_date)
                    full_result["phases"]["calendar_update"] = calendar_result
                    full_result["summary"]["total_phases"] += 1
                    # æ›´æ–°è¿›åº¦æ¡
                    if pbar is not None:
                        pbar.update(1)

                    if "error" not in calendar_result:
                        full_result["summary"]["successful_phases"] += 1
                        updated_records = calendar_result.get("updated_records", 0)
                        total_records = calendar_result.get("total_records", 0)
                        years_range = f"{calendar_result.get('start_year')}-{calendar_result.get('end_year')}"
                        log_phase_complete(
                            "äº¤æ˜“æ—¥å†æ›´æ–°",
                            {
                                "å¹´ä»½èŒƒå›´": years_range,
                                "æ–°å¢è®°å½•": f"{updated_records}æ¡",
                                "æ€»è®°å½•": f"{total_records}æ¡",
                            },
                        )
                    else:
                        full_result["summary"]["failed_phases"] += 1
                        log_error(f"äº¤æ˜“æ—¥å†æ›´æ–°å¤±è´¥: {calendar_result['error']}")

                    # æ›´æ–°è‚¡ç¥¨åˆ—è¡¨
                    update_phase_description("æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰")
                    stock_list_result = self._update_stock_list(target_date)
                    full_result["phases"]["stock_list_update"] = stock_list_result
                    full_result["summary"]["total_phases"] += 1
                    # æ›´æ–°è¿›åº¦æ¡
                    if pbar is not None:
                        pbar.update(1)

                    if "error" not in stock_list_result:
                        full_result["summary"]["successful_phases"] += 1
                        total_stocks = stock_list_result.get("total_stocks", 0)
                        new_stocks = stock_list_result.get("new_stocks", 0)
                        updated_stocks = stock_list_result.get("updated_stocks", 0)
                        log_phase_complete(
                            "è‚¡ç¥¨åˆ—è¡¨æ›´æ–°",
                            {
                                "æ€»è‚¡ç¥¨": f"{total_stocks}åª",
                                "æ–°å¢": f"{new_stocks}åª",
                                "æ›´æ–°": f"{updated_stocks}åª",
                            },
                        )
                    else:
                        full_result["summary"]["failed_phases"] += 1
                        log_error(f"è‚¡ç¥¨åˆ—è¡¨æ›´æ–°å¤±è´¥: {stock_list_result['error']}")

                except Exception as e:
                    log_error(f"åŸºç¡€æ•°æ®æ›´æ–°å¤±è´¥: {e}")
                    full_result["phases"]["base_data_update"] = {"error": str(e)}
                    full_result["summary"]["total_phases"] += 1
                    full_result["summary"]["failed_phases"] += 1

            # å¦‚æœæ²¡æœ‰æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œä»æ•°æ®åº“è·å–æ´»è·ƒè‚¡ç¥¨ï¼ˆå®Œæ•´æµç¨‹éœ€è¦ï¼‰
            if not symbols:
                symbols = self._get_active_stocks_from_db()
                if not symbols:
                    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰è‚¡ç¥¨ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨
                    symbols = ["000001.SZ", "000002.SZ", "600000.SS", "600036.SS"]
                    self.logger.info(f"ä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨: {len(symbols)}åªè‚¡ç¥¨")
                else:
                    self.logger.info(f"ä»æ•°æ®åº“è·å–æ´»è·ƒè‚¡ç¥¨: {len(symbols)}åªè‚¡ç¥¨")

            # é˜¶æ®µ1: å¢é‡åŒæ­¥ï¼ˆå¸‚åœºæ•°æ®ï¼‰
            log_phase_start("é˜¶æ®µ1", "å¢é‡åŒæ­¥å¸‚åœºæ•°æ®")

            with create_phase_progress(
                "phase1", len(symbols), "å¢é‡åŒæ­¥", "è‚¡ç¥¨"
            ) as pbar:
                try:
                    # ä¿®æ”¹å¢é‡åŒæ­¥ä»¥æ”¯æŒè¿›åº¦å›è°ƒ
                    sync_result = self.incremental_sync.sync_all_symbols(
                        target_date, symbols, frequencies, progress_bar=pbar
                    )
                    full_result["phases"]["incremental_sync"] = {
                        "status": "completed",
                        "result": sync_result,
                    }
                    full_result["summary"]["successful_phases"] += 1

                    # ä»ç»“æœä¸­æå–ç»Ÿè®¡ä¿¡æ¯
                    success_count = sync_result.get("success_count", len(symbols))
                    error_count = sync_result.get("error_count", 0)
                    log_phase_complete(
                        "å¢é‡åŒæ­¥",
                        {"æˆåŠŸ": f"{success_count}åªè‚¡ç¥¨", "å¤±è´¥": error_count},
                    )

                except Exception as e:
                    log_error(f"å¢é‡åŒæ­¥å¤±è´¥: {e}")
                    full_result["phases"]["incremental_sync"] = {
                        "status": "failed",
                        "error": str(e),
                    }
                    full_result["summary"]["failed_phases"] += 1

            full_result["summary"]["total_phases"] += 1

            # é˜¶æ®µ2: åŒæ­¥æ‰©å±•æ•°æ®
            log_phase_start("é˜¶æ®µ2", "åŒæ­¥æ‰©å±•æ•°æ®")

            # é¢„æ£€æŸ¥æ‰©å±•æ•°æ®åŒæ­¥çš„æ–­ç‚¹ç»­ä¼ çŠ¶æ€
            extended_symbols_to_process = self._get_extended_data_symbols_to_process(
                symbols, target_date
            )

            self.logger.info(
                f"ğŸ“Š æ‰©å±•æ•°æ®åŒæ­¥: æ€»è‚¡ç¥¨ {len(symbols)}åª, éœ€å¤„ç† {len(extended_symbols_to_process)}åª"
            )

            # å¦‚æœæ²¡æœ‰è‚¡ç¥¨éœ€è¦å¤„ç†ï¼Œç›´æ¥è·³è¿‡
            if len(extended_symbols_to_process) == 0:
                self.logger.info("âœ… æ‰€æœ‰è‚¡ç¥¨çš„æ‰©å±•æ•°æ®å·²å®Œæˆï¼Œè·³è¿‡æ‰©å±•æ•°æ®åŒæ­¥")
                full_result["phases"]["extended_data_sync"] = {
                    "status": "skipped",
                    "result": {"message": "æ‰€æœ‰æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€å¤„ç†"},
                }
                full_result["summary"]["successful_phases"] += 1
                log_phase_complete("æ‰©å±•æ•°æ®åŒæ­¥", {"çŠ¶æ€": "å·²å®Œæˆï¼Œè·³è¿‡"})
            else:
                # å¤„ç†æ‰€æœ‰éœ€è¦çš„è‚¡ç¥¨ï¼Œä¸è®¾é™åˆ¶
                actual_symbols_to_process = extended_symbols_to_process
                self.logger.info(
                    f"ğŸ¯ å¼€å§‹å¤„ç†å…¨éƒ¨ {len(extended_symbols_to_process)} åªéœ€è¦å¤„ç†çš„è‚¡ç¥¨"
                )

                # ä½¿ç”¨æ‰€æœ‰éœ€è¦å¤„ç†çš„è‚¡ç¥¨æ•°é‡ä½œä¸ºè¿›åº¦æ¡åŸºå‡†
                with create_phase_progress(
                    "phase2", len(actual_symbols_to_process), "æ‰©å±•æ•°æ®åŒæ­¥", "è‚¡ç¥¨"
                ) as pbar:
                    try:
                        extended_result = self._sync_extended_data(
                            actual_symbols_to_process,  # ä½¿ç”¨å®é™…è¦å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨
                            target_date,
                            pbar,  # ä¼ å…¥æ­£ç¡®å¤§å°çš„è¿›åº¦æ¡
                        )
                        full_result["phases"]["extended_data_sync"] = {
                            "status": "completed",
                            "result": extended_result,
                        }
                        full_result["summary"]["successful_phases"] += 1

                        log_phase_complete(
                            "æ‰©å±•æ•°æ®åŒæ­¥",
                            {
                                "è´¢åŠ¡æ•°æ®": f"{extended_result.get('financials_count', 0)}æ¡",
                                "ä¼°å€¼æ•°æ®": f"{extended_result.get('valuations_count', 0)}æ¡",
                                "æŠ€æœ¯æŒ‡æ ‡": f"{extended_result.get('indicators_count', 0)}æ¡",
                            },
                        )

                    except Exception as e:
                        log_error(f"æ‰©å±•æ•°æ®åŒæ­¥å¤±è´¥: {e}")
                        full_result["phases"]["extended_data_sync"] = {
                            "status": "failed",
                            "error": str(e),
                        }
                        full_result["summary"]["failed_phases"] += 1

            full_result["summary"]["total_phases"] += 1

            # é˜¶æ®µ3: ç¼ºå£æ£€æµ‹
            log_phase_start("é˜¶æ®µ3", "ç¼ºå£æ£€æµ‹ä¸ä¿®å¤")

            with create_phase_progress(
                "phase2", len(symbols), "ç¼ºå£æ£€æµ‹", "è‚¡ç¥¨"
            ) as pbar:
                try:
                    gap_start_date = target_date - timedelta(days=30)  # æ£€æµ‹æœ€è¿‘30å¤©
                    gap_result = self.gap_detector.detect_all_gaps(
                        gap_start_date, target_date, symbols, frequencies
                    )

                    # æ›´æ–°è¿›åº¦
                    # æ›´æ–°è¿›åº¦
                    if pbar is not None:
                        pbar.update(len(symbols))

                    full_result["phases"]["gap_detection"] = {
                        "status": "completed",
                        "result": gap_result,
                    }
                    full_result["summary"]["successful_phases"] += 1

                    total_gaps = gap_result["summary"]["total_gaps"]

                    # è‡ªåŠ¨ä¿®å¤ç¼ºå£
                    if self.enable_auto_gap_fix and total_gaps > 0:
                        update_phase_description(f"ä¿®å¤{total_gaps}ä¸ªç¼ºå£")
                        fix_result = self._auto_fix_gaps(gap_result)
                        full_result["phases"]["gap_fix"] = {
                            "status": "completed",
                            "result": fix_result,
                        }
                        log_phase_complete(
                            "ç¼ºå£æ£€æµ‹ä¸ä¿®å¤",
                            {"æ£€æµ‹": f"{total_gaps}ä¸ªç¼ºå£", "ä¿®å¤": "å®Œæˆ"},
                        )
                    else:
                        log_phase_complete("ç¼ºå£æ£€æµ‹", {"ç¼ºå£": f"{total_gaps}ä¸ª"})

                except Exception as e:
                    log_error(f"ç¼ºå£æ£€æµ‹å¤±è´¥: {e}")
                    full_result["phases"]["gap_detection"] = {
                        "status": "failed",
                        "error": str(e),
                    }
                    full_result["summary"]["failed_phases"] += 1

            full_result["summary"]["total_phases"] += 1

            # é˜¶æ®µ3: æ•°æ®éªŒè¯
            if self.enable_validation:
                log_phase_start("é˜¶æ®µ3", "æ•°æ®éªŒè¯")

                with create_phase_progress(
                    "phase3", len(symbols), "æ•°æ®éªŒè¯", "è‚¡ç¥¨"
                ) as pbar:
                    try:
                        validation_start_date = target_date - timedelta(
                            days=7
                        )  # éªŒè¯æœ€è¿‘7å¤©
                        validation_result = self.validator.validate_all_data(
                            validation_start_date, target_date, symbols, frequencies
                        )

                        # æ›´æ–°è¿›åº¦
                        if pbar is not None:
                            pbar.update(len(symbols))

                        full_result["phases"]["validation"] = {
                            "status": "completed",
                            "result": validation_result,
                        }
                        full_result["summary"]["successful_phases"] += 1

                        # æå–éªŒè¯ç»Ÿè®¡
                        total_records = validation_result.get("total_records", 0)
                        valid_records = validation_result.get("valid_records", 0)
                        validation_rate = validation_result.get("validation_rate", 0)

                        log_phase_complete(
                            "æ•°æ®éªŒè¯",
                            {
                                "è®°å½•": f"{total_records}æ¡",
                                "æœ‰æ•ˆ": f"{valid_records}æ¡",
                                "éªŒè¯ç‡": f"{validation_rate:.1f}%",
                            },
                        )

                    except Exception as e:
                        log_error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
                        full_result["phases"]["validation"] = {
                            "status": "failed",
                            "error": str(e),
                        }
                        full_result["summary"]["failed_phases"] += 1

                full_result["summary"]["total_phases"] += 1

            # å®Œæˆæ—¶é—´
            end_time = datetime.now()
            full_result["end_time"] = end_time.isoformat()
            full_result["duration_seconds"] = (end_time - start_time).total_seconds()

            self._log_performance(
                "run_full_sync",
                full_result["duration_seconds"],
                successful_phases=full_result["summary"]["successful_phases"],
                failed_phases=full_result["summary"]["failed_phases"],
            )

            return full_result

        except Exception as e:
            self._log_error("run_full_sync", e, target_date=target_date)
            raise

    def get_sync_status(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥çŠ¶æ€"""
        try:
            # è·å–æœ€è¿‘çš„åŒæ­¥çŠ¶æ€
            sql = """
            SELECT * FROM sync_status
            ORDER BY last_sync_date DESC
            LIMIT 10
            """
            recent_syncs = self.db_manager.fetchall(sql)

            # è·å–æ•°æ®ç»Ÿè®¡
            stats_sql = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT symbol) as total_symbols,
                COUNT(DISTINCT date) as total_dates,
                MIN(date) as earliest_date,
                MAX(date) as latest_date,
                AVG(quality_score) as avg_quality
            FROM market_data
            """
            stats_result = self.db_manager.fetchone(stats_sql)

            # è·å–ç»„ä»¶çŠ¶æ€
            components_status = {
                "incremental_sync": {
                    "initialized": hasattr(self, "incremental_sync")
                    and self.incremental_sync is not None,
                    "type": "IncrementalSync",
                },
                "gap_detector": {
                    "initialized": hasattr(self, "gap_detector")
                    and self.gap_detector is not None,
                    "type": "GapDetector",
                },
                "validator": {
                    "initialized": hasattr(self, "validator")
                    and self.validator is not None,
                    "type": "DataValidator",
                },
            }

            # è¿”å›æ ‡å‡†æ ¼å¼
            return {
                "success": True,
                "data": {
                    "recent_syncs": [dict(row) for row in recent_syncs],
                    "data_stats": dict(stats_result) if stats_result else {},
                    "components": components_status,
                    "config": {
                        "enable_auto_gap_fix": self.enable_auto_gap_fix,
                        "enable_validation": self.enable_validation,
                        "max_gap_fix_days": self.max_gap_fix_days,
                    },
                },
            }
        except Exception as e:
            self.logger.error(f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def _get_active_stocks_from_db(self) -> List[str]:
        """ä»æ•°æ®åº“è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨"""
        sql = "SELECT symbol FROM stocks WHERE status = 'active' ORDER BY symbol"
        result = self.db_manager.fetchall(sql)
        return [row["symbol"] for row in result] if result else []

    def _get_extended_data_symbols_to_process(
        self, symbols: List[str], target_date: date
    ) -> List[str]:
        """
        è·å–éœ€è¦å¤„ç†æ‰©å±•æ•°æ®çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ™ºèƒ½æ–­ç‚¹ç»­ä¼ ç‰ˆæœ¬ï¼‰
        """
        try:
            self.logger.info("ğŸ“Š æ£€æŸ¥æ‰©å±•æ•°æ®å®Œæ•´æ€§ï¼ˆæ™ºèƒ½æ–­ç‚¹ç»­ä¼ ï¼‰...")

            if not symbols:
                return []

            # æ¸…ç†è¿‡æœŸçš„pendingçŠ¶æ€
            cleanup_count = self.db_manager.execute(
                """
                DELETE FROM extended_sync_status 
                WHERE target_date = ? AND status = 'pending' 
                AND created_at < datetime('now', '-1 day')
                """,
                (str(target_date),),
            )

            # æ™ºèƒ½æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼šä½¿ç”¨çµæ´»çš„æ—¥æœŸèŒƒå›´
            # è´¢åŠ¡æ•°æ®ï¼šæ£€æŸ¥æœ€è¿‘2å¹´çš„å¹´æŠ¥æ•°æ®
            financial_dates = [
                f"{target_date.year - 1}-12-31",  # å»å¹´å¹´æŠ¥
                f"{target_date.year - 2}-12-31",  # å‰å¹´å¹´æŠ¥ï¼ˆå¤‡ç”¨ï¼‰
            ]

            # ä¼°å€¼æ•°æ®ï¼šæ£€æŸ¥ç›®æ ‡æ—¥æœŸå‰å10å¤©èŒƒå›´
            from datetime import timedelta

            valuation_start = str(target_date - timedelta(days=10))
            valuation_end = str(target_date + timedelta(days=10))

            placeholders = ",".join(["?" for _ in symbols])
            financial_placeholders = ",".join(["?" for _ in financial_dates])

            # çµæ´»çš„æ•°æ®å®Œæ•´æ€§æŸ¥è¯¢
            data_completeness_query = f"""
            WITH symbol_list AS (
                SELECT symbol FROM stocks 
                WHERE symbol IN ({placeholders}) AND status = 'active'
            ),
            financial_data AS (
                SELECT DISTINCT symbol FROM financials 
                WHERE symbol IN ({placeholders}) 
                AND report_date IN ({financial_placeholders})
            ),
            valuation_data AS (
                SELECT DISTINCT symbol FROM valuations 
                WHERE symbol IN ({placeholders})
                AND date BETWEEN ? AND ?
            ),
            status_data AS (
                SELECT DISTINCT symbol, status FROM extended_sync_status
                WHERE symbol IN ({placeholders}) 
                AND target_date = ? AND status = 'completed'
            )
            SELECT 
                sl.symbol,
                CASE WHEN fd.symbol IS NOT NULL THEN 1 ELSE 0 END AS has_financial,
                CASE WHEN vd.symbol IS NOT NULL THEN 1 ELSE 0 END AS has_valuation,
                CASE WHEN sd.symbol IS NOT NULL THEN 1 ELSE 0 END AS marked_completed
            FROM symbol_list sl
            LEFT JOIN financial_data fd ON sl.symbol = fd.symbol
            LEFT JOIN valuation_data vd ON sl.symbol = vd.symbol  
            LEFT JOIN status_data sd ON sl.symbol = sd.symbol
            """

            # æ‰§è¡ŒæŸ¥è¯¢
            query_params = (
                tuple(symbols)
                + tuple(symbols)
                + tuple(financial_dates)
                + tuple(symbols)
                + (valuation_start, valuation_end)
                + tuple(symbols)
                + (str(target_date),)
            )
            results = self.db_manager.fetchall(data_completeness_query, query_params)

            # æ™ºèƒ½åˆ†æç»“æœå¹¶ä¿®å¤çŠ¶æ€
            symbols_needing_processing = []
            repaired_symbols = []  # çŠ¶æ€ä¿®å¤çš„è‚¡ç¥¨
            stats = {
                "total_checked": len(results),
                "completed": 0,
                "partial": 0,
                "missing": 0,
                "needs_processing": 0,
                "status_repaired": 0,
            }

            for row in results:
                symbol = row["symbol"]
                has_financial = row["has_financial"]
                has_valuation = row["has_valuation"]
                marked_completed = row["marked_completed"]

                # è¯„ä¼°å®é™…å®ŒæˆçŠ¶æ€ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
                # å…³é”®ä¿®å¤ï¼šæ²¡æœ‰åŒæ­¥è®°å½•çš„è‚¡ç¥¨å¿…é¡»å¤„ç†ï¼Œæ— è®ºæ˜¯å¦æœ‰æ•°æ®
                if not marked_completed:
                    # æ²¡æœ‰åŒæ­¥è®°å½•ï¼Œå¿…é¡»å¤„ç†
                    actual_status = "pending"
                    stats["missing"] += 1
                elif has_financial:
                    actual_status = "completed"  # æœ‰è´¢åŠ¡æ•°æ®ä¸”å·²æ ‡è®°å®Œæˆ
                    stats["completed"] += 1
                elif has_valuation:
                    actual_status = "partial"  # åªæœ‰ä¼°å€¼æ•°æ®
                    stats["partial"] += 1
                else:
                    actual_status = "pending"  # æ ‡è®°å®Œæˆä½†æ²¡æœ‰æ•°æ®ï¼Œéœ€è¦é‡æ–°å¤„ç†
                    stats["missing"] += 1

                # æ™ºèƒ½çŠ¶æ€ä¿®å¤ï¼šä¿®å¤è€Œä¸æ˜¯åˆ é™¤
                if marked_completed and actual_status != "completed":
                    # çŠ¶æ€ä¸ä¸€è‡´ï¼Œéœ€è¦ä¿®å¤
                    self.db_manager.execute(
                        "UPDATE extended_sync_status SET status = ?, updated_at = datetime('now') WHERE symbol = ? AND target_date = ?",
                        (actual_status, symbol, str(target_date)),
                    )
                    repaired_symbols.append(symbol)
                    stats["status_repaired"] += 1
                    self.logger.debug(
                        f"ğŸ”§ ä¿®å¤çŠ¶æ€: {symbol} completed -> {actual_status} (è´¢åŠ¡:{has_financial}, ä¼°å€¼:{has_valuation})"
                    )

                # éœ€è¦å¤„ç†çš„æ¡ä»¶ï¼šå®é™…çŠ¶æ€ä¸æ˜¯å®Œæˆ
                if actual_status != "completed":
                    symbols_needing_processing.append(symbol)
                    stats["needs_processing"] += 1

            # è¾“å‡ºæ™ºèƒ½ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(
                f"ğŸ“Š æ™ºèƒ½æ•°æ®å®Œæ•´æ€§æ£€æŸ¥: "
                f"æ€»è®¡{stats['total_checked']}, "
                f"å·²å®Œæˆ{stats['completed']}, "
                f"éƒ¨åˆ†å®Œæˆ{stats['partial']}, "
                f"ç¼ºå¤±æ•°æ®{stats['missing']}, "
                f"éœ€å¤„ç†{stats['needs_processing']}, "
                f"çŠ¶æ€ä¿®å¤{stats['status_repaired']}"
            )

            if repaired_symbols:
                self.logger.info(
                    f"ğŸ”§ çŠ¶æ€ä¿®å¤: {len(repaired_symbols)} ä¸ªè‚¡ç¥¨çŠ¶æ€å·²ä¿®å¤"
                )

            if symbols_needing_processing:
                completion_rate = (
                    (stats["total_checked"] - len(symbols_needing_processing))
                    / stats["total_checked"]
                    if stats["total_checked"] > 0
                    else 0
                )
                self.logger.info(
                    f"ğŸ“‹ æ–­ç‚¹ç»­ä¼ : æ€»è¿›åº¦ {completion_rate:.1%}, å‰©ä½™å¤„ç† {len(symbols_needing_processing)} åªè‚¡ç¥¨"
                )
            else:
                self.logger.info("âœ… æ‰€æœ‰è‚¡ç¥¨çš„æ‰©å±•æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€å¤„ç†")

            return symbols_needing_processing

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ‰©å±•æ•°æ®å®Œæ•´æ€§å¤±è´¥: {e}")
            raise

    def _update_trading_calendar(self, target_date: date) -> Dict[str, Any]:
        """å¢é‡æ›´æ–°äº¤æ˜“æ—¥å†"""
        self.logger.info(f"ğŸ”„ å¼€å§‹äº¤æ˜“æ—¥å†å¢é‡æ›´æ–°ï¼Œç›®æ ‡æ—¥æœŸ: {target_date}")

        # æ£€æŸ¥ç°æœ‰æ•°æ®èŒƒå›´
        existing_range = self.db_manager.fetchone(
            "SELECT MIN(date) as min_date, MAX(date) as max_date, COUNT(*) as count FROM trading_calendar"
        )

        # è®¡ç®—éœ€è¦æ›´æ–°çš„å¹´ä»½
        needed_start_year = target_date.year - 1
        needed_end_year = target_date.year + 1
        years_to_update = list(range(needed_start_year, needed_end_year + 1))

        if existing_range and existing_range["count"] > 0:
            from datetime import datetime

            existing_min = datetime.strptime(
                existing_range["min_date"], "%Y-%m-%d"
            ).date()
            existing_max = datetime.strptime(
                existing_range["max_date"], "%Y-%m-%d"
            ).date()

            # åªæ·»åŠ ç¼ºå¤±çš„å¹´ä»½
            years_to_update = [
                y
                for y in years_to_update
                if y < existing_min.year or y > existing_max.year
            ]

            if not years_to_update:
                return {
                    "status": "skipped",
                    "message": "äº¤æ˜“æ—¥å†å·²æ˜¯æœ€æ–°",
                    "start_year": existing_min.year,
                    "end_year": existing_max.year,
                    "updated_records": 0,
                    "total_records": existing_range["count"],
                }

        self.logger.info(f"éœ€è¦æ›´æ–°å¹´ä»½: {years_to_update}")
        total_inserted = 0

        # è·å–å¹¶æ’å…¥æ•°æ®
        for year in years_to_update:
            start_date = f"{year}-01-01"
            end_date = f"{year}-12-31"

            calendar_data = self.data_source_manager.get_trade_calendar(
                start_date, end_date
            )

            self.logger.debug(
                f"è·å–åˆ°äº¤æ˜“æ—¥å†åŸå§‹æ•°æ®: {type(calendar_data)}, å†…å®¹: {calendar_data}"
            )

            # å¤„ç†åµŒå¥—æ ¼å¼
            if isinstance(calendar_data, dict) and "success" in calendar_data:
                if calendar_data.get("success"):
                    if "data" in calendar_data:
                        calendar_data = calendar_data["data"]
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ·±å±‚åµŒå¥—
                        while (
                            isinstance(calendar_data, dict)
                            and "success" in calendar_data
                            and calendar_data.get("success")
                            and "data" in calendar_data
                        ):
                            calendar_data = calendar_data["data"]
                else:
                    self.logger.warning(
                        f"äº¤æ˜“æ—¥å†è·å–å¤±è´¥: {calendar_data.get('message', 'æœªçŸ¥é”™è¯¯')}"
                    )
                    continue
            elif isinstance(calendar_data, dict) and "data" in calendar_data:
                calendar_data = calendar_data["data"]

            self.logger.debug(
                f"å¤„ç†åäº¤æ˜“æ—¥å†æ•°æ®: {type(calendar_data)}, é•¿åº¦: {len(calendar_data) if isinstance(calendar_data, list) else 'N/A'}"
            )

            if not calendar_data or not isinstance(calendar_data, list):
                continue

            # æ‰¹é‡æ’å…¥æ•°æ®ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            records_to_insert = [
                (
                    record.get("trade_date", record.get("date")),
                    "CN",
                    record.get("is_trading", 1),
                )
                for record in calendar_data
            ]

            if records_to_insert:
                self.db_manager.executemany(
                    "INSERT OR REPLACE INTO trading_calendar (date, market, is_trading) VALUES (?, ?, ?)",
                    records_to_insert,
                )
                total_inserted += len(records_to_insert)

        # éªŒè¯ç»“æœ
        final_range = self.db_manager.fetchone(
            "SELECT MIN(date) as min_date, MAX(date) as max_date, COUNT(*) as count FROM trading_calendar"
        )

        return {
            "status": "completed",
            "start_year": (
                int(final_range["min_date"][:4])
                if final_range and final_range["min_date"]
                else needed_start_year
            ),
            "end_year": (
                int(final_range["max_date"][:4])
                if final_range and final_range["max_date"]
                else needed_end_year
            ),
            "updated_records": total_inserted,
            "total_records": final_range["count"] if final_range else 0,
        }

    def _update_stock_list(self, target_date: Optional[date] = None) -> Dict[str, Any]:
        """
        å¢é‡æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œç”¨äºè·å–è¯¥æ—¥æœŸçš„è‚¡ç¥¨åˆ—è¡¨
        """
        if target_date is None:
            target_date = datetime.now().date()

        self.logger.info("ğŸ”„ å¼€å§‹è‚¡ç¥¨åˆ—è¡¨å¢é‡æ›´æ–°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")

        try:
            # å¢é‡ç­–ç•¥ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            last_update = self.db_manager.fetchone(
                "SELECT MAX(updated_at) as last_update FROM stocks WHERE status = 'active'"
            )

            # å¦‚æœä»Šå¤©å·²ç»æ›´æ–°è¿‡ï¼Œä¸”è‚¡ç¥¨æ•°é‡åˆç†ï¼Œè·³è¿‡æ›´æ–°
            from datetime import datetime, timedelta

            today = datetime.now().date()

            if last_update and last_update["last_update"]:
                last_update_date = datetime.fromisoformat(
                    last_update["last_update"]
                ).date()
                stock_count = self.db_manager.fetchone(
                    "SELECT COUNT(*) as count FROM stocks WHERE status = 'active'"
                )

                # å¦‚æœä»Šå¤©å·²æ›´æ–°è¿‡ä¸”è‚¡ç¥¨æ•°é‡ > 3000ï¼Œè·³è¿‡
                if (
                    last_update_date >= today
                    and stock_count
                    and stock_count["count"] > 3000
                ):
                    self.logger.info(
                        f"âœ… è‚¡ç¥¨åˆ—è¡¨ä»Šæ—¥å·²æ›´æ–°ï¼Œå…± {stock_count['count']} åªè‚¡ç¥¨ï¼Œè·³è¿‡æ›´æ–°"
                    )
                    return {
                        "status": "skipped",
                        "message": "ä»Šæ—¥å·²æ›´æ–°ï¼Œè·³è¿‡",
                        "total_stocks": stock_count["count"],
                        "new_stocks": 0,
                        "updated_stocks": 0,
                        "failed_stocks": 0,
                    }
                # å¢åŠ ä¸€ä¸ªæ›´å®½æ¾çš„è·³è¿‡æ¡ä»¶ - å¦‚æœè‚¡ç¥¨æ•°é‡ > 1000ä¸”æœ€è¿‘æ›´æ–°è¿‡
                elif (
                    last_update_date >= (today - timedelta(days=1))  # 1å¤©å†…æ›´æ–°è¿‡
                    and stock_count
                    and stock_count["count"] > 1000
                ):
                    self.logger.info(
                        f"âœ… è‚¡ç¥¨åˆ—è¡¨æœ€è¿‘å·²æ›´æ–°ï¼ˆ{last_update_date}ï¼‰ï¼Œå…± {stock_count['count']} åªè‚¡ç¥¨ï¼Œè·³è¿‡æ›´æ–°ä»¥æé«˜æ€§èƒ½"
                    )
                    return {
                        "status": "skipped",
                        "message": "æœ€è¿‘å·²æ›´æ–°ï¼Œè·³è¿‡",
                        "total_stocks": stock_count["count"],
                        "new_stocks": 0,
                        "updated_stocks": 0,
                        "failed_stocks": 0,
                    }

            # è·å–è‚¡ç¥¨ä¿¡æ¯ - ä½¿ç”¨ç›®æ ‡æ—¥æœŸçš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆé¿å…å¹¸å­˜è€…åå·®ï¼‰
            # ç›´æ¥ä½¿ç”¨ BaoStock ä»¥æ”¯æŒå†å²æ—¥æœŸæŸ¥è¯¢
            self.logger.info(f"ğŸ”„ å¼€å§‹è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆç›®æ ‡æ—¥æœŸ: {target_date}ï¼‰...")
            baostock_source = self.data_source_manager.get_source("baostock")
            if not baostock_source:
                raise ValidationError("BaoStockæ•°æ®æºä¸å¯ç”¨")

            if not baostock_source.is_connected():
                baostock_source.connect()

            # BaoStock æ”¯æŒæŒ‡å®šæ—¥æœŸæŸ¥è¯¢ï¼Œç¡®ä¿è·å–ç›®æ ‡æ—¥æœŸçš„è‚¡ç¥¨åˆ—è¡¨
            # ä¿®æ”¹ get_stock_info ä»¥æ”¯æŒæ—¥æœŸå‚æ•°
            stock_info = baostock_source.get_stock_info(target_date=str(target_date))

            # BaoStockç›´æ¥è¿”å›åˆ—è¡¨ï¼ŒéªŒè¯æ•°æ®æ ¼å¼
            if not isinstance(stock_info, list):
                self.logger.error(f"BaoStockè¿”å›æ ¼å¼é”™è¯¯: {type(stock_info)}")
                return {
                    "status": "failed",
                    "error": f"BaoStockè¿”å›æ ¼å¼é”™è¯¯: {type(stock_info)}",
                    "total_stocks": 0,
                    "new_stocks": 0,
                    "updated_stocks": 0,
                }

            if not stock_info:
                self.logger.warning("BaoStockè¿”å›ç©ºåˆ—è¡¨")
                return {
                    "status": "failed",
                    "error": "è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼šBaoStockè¿”å›ç©ºåˆ—è¡¨",
                    "total_stocks": 0,
                    "new_stocks": 0,
                    "updated_stocks": 0,
                }

            self.logger.info(f"âœ… ä»BaoStockè·å– {len(stock_info)} åªè‚¡ç¥¨")

            # æ‰¹é‡å¤„ç†è‚¡ç¥¨æ•°æ® - æ€§èƒ½ä¼˜åŒ–
            new_stocks = 0
            updated_stocks = 0
            failed_stocks = 0

            # é¢„å¤„ç†æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            processed_stocks = []

            for i, stock_data in enumerate(stock_info):
                try:
                    # æ£€æŸ¥æ•°æ®ç±»å‹
                    if not isinstance(stock_data, dict):
                        if i < 5:  # åªè®°å½•å‰5ä¸ªé”™è¯¯
                            self.logger.warning(
                                f"ç¬¬{i}ä¸ªè‚¡ç¥¨æ•°æ®ä¸æ˜¯å­—å…¸: ç±»å‹={type(stock_data)}, å†…å®¹={stock_data}"
                            )
                        failed_stocks += 1
                        continue

                    symbol = stock_data.get("symbol", "")
                    name = stock_data.get("name", "")
                    market = stock_data.get("market", "")

                    if not symbol or not name:
                        continue

                    # æ·»åŠ å¸‚åœºåç¼€
                    if "." not in symbol:
                        if symbol.startswith("0") or symbol.startswith("3"):
                            symbol = f"{symbol}.SZ"
                        elif symbol.startswith("6") or symbol.startswith("9"):
                            symbol = f"{symbol}.SS"

                    processed_stocks.append(
                        {"symbol": symbol, "name": name, "market": market}
                    )

                except Exception as e:
                    if failed_stocks < 5:  # åªè®°å½•å‰5ä¸ªé”™è¯¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                        self.logger.error(f"é¢„å¤„ç†ç¬¬{i}ä¸ªè‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
                    failed_stocks += 1

            if not processed_stocks:
                self.logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®éœ€è¦å¤„ç†")
                return {
                    "status": "completed",
                    "total_stocks": 0,
                    "new_stocks": 0,
                    "updated_stocks": 0,
                    "failed_stocks": failed_stocks,
                }

            # æ‰¹é‡æ£€æŸ¥å·²å­˜åœ¨çš„è‚¡ç¥¨
            symbol_list = [stock["symbol"] for stock in processed_stocks]
            placeholders = ",".join(["?" for _ in symbol_list])
            existing_symbols = set()

            try:
                existing_result = self.db_manager.fetchall(
                    f"SELECT symbol FROM stocks WHERE symbol IN ({placeholders})",
                    tuple(symbol_list),
                )
                existing_symbols = {row["symbol"] for row in existing_result}
                self.logger.debug(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(existing_symbols)} åªè‚¡ç¥¨")
            except Exception as e:
                self.logger.warning(f"æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨è‚¡ç¥¨å¤±è´¥: {e}")
                # å›é€€åˆ°é€ä¸€å¤„ç†
                existing_symbols = set()

            # åˆ†ç¦»æ–°è‚¡ç¥¨å’Œéœ€è¦æ›´æ–°çš„è‚¡ç¥¨
            new_stock_batch = []
            update_stock_batch = []

            for stock in processed_stocks:
                if stock["symbol"] in existing_symbols:
                    update_stock_batch.append((stock["name"], stock["symbol"]))
                else:
                    new_stock_batch.append(
                        (
                            stock["symbol"],
                            stock["name"],
                            stock["market"],
                            stock["market"],  # exchange å­—æ®µ
                            "active",
                        )
                    )

            # æ‰¹é‡æ›´æ–°å·²å­˜åœ¨çš„è‚¡ç¥¨
            if update_stock_batch:
                try:
                    self.db_manager.executemany(
                        "UPDATE stocks SET name = ?, updated_at = datetime('now') WHERE symbol = ?",
                        update_stock_batch,
                    )
                    updated_stocks = len(update_stock_batch)
                    self.logger.debug(f"æ‰¹é‡æ›´æ–° {updated_stocks} åªè‚¡ç¥¨")
                except Exception as e:
                    self.logger.warning(f"æ‰¹é‡æ›´æ–°è‚¡ç¥¨å¤±è´¥: {e}")
                    # é€ä¸€æ›´æ–°
                    for name, symbol in update_stock_batch:
                        try:
                            self.db_manager.execute(
                                "UPDATE stocks SET name = ?, updated_at = datetime('now') WHERE symbol = ?",
                                (name, symbol),
                            )
                            updated_stocks += 1
                        except Exception as e2:
                            self.logger.warning(f"æ›´æ–°è‚¡ç¥¨ {symbol} å¤±è´¥: {e2}")
                            failed_stocks += 1

            # æ‰¹é‡æ’å…¥æ–°è‚¡ç¥¨
            if new_stock_batch:
                try:
                    self.db_manager.executemany(
                        """
                        INSERT INTO stocks (symbol, name, market, status, created_at, updated_at)
                        VALUES (?, ?, ?, ?, datetime('now'), datetime('now'))
                        """,
                        [(row[0], row[1], row[2], row[4]) for row in new_stock_batch],
                    )
                    new_stocks = len(new_stock_batch)
                    self.logger.debug(f"æ‰¹é‡æ’å…¥ {new_stocks} åªæ–°è‚¡ç¥¨")

                    # ä¸ºæ‰€æœ‰æ–°è‚¡ç¥¨è·å–è¯¦ç»†ä¿¡æ¯
                    for symbol, _, _, _, _ in new_stock_batch:
                        try:
                            self._fetch_detailed_stock_info(symbol)
                        except Exception as e:
                            self.logger.debug(f"è·å– {symbol} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")

                except Exception as e:
                    self.logger.warning(f"æ‰¹é‡æ’å…¥æ–°è‚¡ç¥¨å¤±è´¥: {e}")
                    # å›é€€åˆ°é€ä¸€æ’å…¥
                    for stock_data in new_stock_batch:
                        try:
                            self.db_manager.execute(
                                """
                                INSERT INTO stocks (symbol, name, market, status, created_at, updated_at)
                                VALUES (?, ?, ?, ?, datetime('now'), datetime('now'))
                                """,
                                (
                                    stock_data[0],
                                    stock_data[1],
                                    stock_data[2],
                                    stock_data[4],
                                ),
                            )
                            new_stocks += 1
                        except Exception as e2:
                            self.logger.warning(f"æ’å…¥è‚¡ç¥¨ {stock_data[0]} å¤±è´¥: {e2}")
                            failed_stocks += 1

            total_processed = new_stocks + updated_stocks

            self.logger.info(
                f"è‚¡ç¥¨åˆ—è¡¨æ›´æ–°å®Œæˆ: æ–°å¢ {new_stocks}åª, æ›´æ–° {updated_stocks}åª, å¤±è´¥ {failed_stocks}åª"
            )

            return {
                "status": "completed",
                "total_stocks": total_processed,
                "new_stocks": new_stocks,
                "updated_stocks": updated_stocks,
                "failed_stocks": failed_stocks,
            }

        except Exception as e:
            self.logger.error(f"æ›´æ–°è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "total_stocks": 0,
                "new_stocks": 0,
                "updated_stocks": 0,
            }

    def _determine_market(self, symbol: str) -> str:
        """
        ç¡®å®šè‚¡ç¥¨å¸‚åœºï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆçº¯æ•°å­—ï¼Œä¸å«åç¼€ï¼‰

        Returns:
            å¸‚åœºä»£ç : SS(ä¸Šæµ·) / SZ(æ·±åœ³) / BJ(åŒ—äº¤æ‰€)
        """
        # ä½¿ç”¨ç¼“å­˜æå‡æ€§èƒ½
        if self.enable_cache and symbol in self._market_cache:
            return self._market_cache[symbol]

        if not symbol or not isinstance(symbol, str):
            result = "SZ"  # é»˜è®¤æ·±åœ³
        else:
            # æ¸…ç†è‚¡ç¥¨ä»£ç ï¼Œç§»é™¤å¯èƒ½çš„åç¼€
            clean_symbol = symbol.split(".")[0].strip()

            if not clean_symbol or not clean_symbol.isdigit():
                result = "SZ"  # é»˜è®¤æ·±åœ³
            # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
            elif clean_symbol.startswith(("600", "601", "603", "605", "688", "689")):
                result = "SS"
            # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€
            elif clean_symbol.startswith(("000", "001", "002", "003", "300", "301")):
                result = "SZ"
            # åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€
            elif clean_symbol.startswith(("8", "43", "83")):
                result = "BJ"
            else:
                # å…¶ä»–æƒ…å†µï¼Œæ ¹æ®é¦–ä½æ•°å­—åˆ¤æ–­
                first_char = clean_symbol[0]
                if first_char == "6":
                    result = "SS"  # ä¸Šæµ·
                elif first_char in ["0", "3"]:
                    result = "SZ"  # æ·±åœ³
                elif first_char == "8":
                    result = "BJ"  # åŒ—äº¤æ‰€
                else:
                    result = "SZ"  # é»˜è®¤æ·±åœ³

        # ç¼“å­˜ç»“æœ
        if self.enable_cache:
            self._market_cache[symbol] = result

        return result

    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if hasattr(self, "_market_cache"):
            self._market_cache.clear()
        if hasattr(self, "_stock_info_cache"):
            self._stock_info_cache.clear()
        self.logger.debug("ç¼“å­˜å·²æ¸…ç†")

    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "market_cache_size": len(getattr(self, "_market_cache", {})),
            "stock_info_cache_size": len(getattr(self, "_stock_info_cache", {})),
        }

    def _fetch_detailed_stock_info(self, symbol: str):
        """è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯ï¼ˆè‚¡æœ¬ã€ä¸Šå¸‚æ—¥æœŸç­‰ï¼‰"""
        try:
            # è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯
            response = self.data_source_manager.get_stock_info(symbol)

            if not response or not isinstance(response, dict):
                self.logger.warning(
                    f"è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯å¤±è´¥: {symbol} - å“åº”ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯"
                )
                return

            # åŒé‡è§£åŒ…æ•°æ®ï¼ˆå› ä¸ºæ˜¯åµŒå¥—æ ¼å¼ï¼‰
            detail_info = self._extract_data_safely(response)
            if (
                isinstance(detail_info, dict)
                and detail_info.get("success")
                and "data" in detail_info
            ):
                detail_info = self._extract_data_safely(detail_info)

            if not detail_info or not isinstance(detail_info, dict):
                self.logger.warning(f"è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯å¤±è´¥: {symbol} - è§£åŒ…åæ•°æ®ä¸ºç©º")
                return

            # æå–å­—æ®µä¿¡æ¯ï¼ˆé€‚é…ä¸åŒæ•°æ®æºçš„å­—æ®µåï¼‰
            total_shares = detail_info.get("total_shares", 0) or 0
            float_shares = detail_info.get("float_shares", 0) or 0
            list_date = detail_info.get("list_date", "")

            # å¤„ç†è¡Œä¸šä¿¡æ¯ - ä¼˜å…ˆä½¿ç”¨å…·ä½“çš„l1/l2å­—æ®µ
            industry_l1 = detail_info.get("industry_l1", "") or detail_info.get(
                "industry", ""
            )
            industry_l2 = detail_info.get("industry_l2", "")

            self.logger.debug(
                f"æå–åˆ°è‚¡ç¥¨ä¿¡æ¯: {symbol} - list_date={list_date}, industry_l1={industry_l1}, industry_l2={industry_l2}"
            )

            # åªè¦æœ‰ä»»ä½•ä¸€ä¸ªæœ‰æ•ˆå­—æ®µå°±æ›´æ–°ï¼ˆä¿®å¤æ¡ä»¶åˆ¤æ–­ï¼‰
            has_data = (
                (total_shares and total_shares > 0)
                or (float_shares and float_shares > 0)
                or (list_date and list_date.strip())
                or (industry_l1 and industry_l1.strip())
                or (industry_l2 and industry_l2.strip())
            )

            if has_data:
                self.db_manager.execute(
                    """
                    UPDATE stocks
                    SET total_shares = ?, float_shares = ?, list_date = ?,
                        industry_l1 = ?, industry_l2 = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE symbol = ?
                    """,
                    (
                        total_shares if total_shares > 0 else None,
                        float_shares if float_shares > 0 else None,
                        list_date if list_date and list_date.strip() else None,
                        industry_l1 if industry_l1 and industry_l1.strip() else None,
                        industry_l2 if industry_l2 and industry_l2.strip() else None,
                        symbol,
                    ),
                )
                self.logger.info(f"âœ… æ›´æ–°è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯: {symbol}")
            else:
                self.logger.warning(f"âš ï¸  è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯ä¸ºç©º: {symbol}")

        except Exception as e:
            self.logger.error(f"è·å– {symbol} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {symbol}", exc_info=True)

    def _safe_extract_number(
        self, value: Any, default: Optional[float] = None
    ) -> Optional[float]:
        """
        å®‰å…¨æå–æ•°å­—ï¼Œæ”¯æŒä¸­æ–‡å•ä½è½¬æ¢

        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: é»˜è®¤å€¼

        Returns:
            è½¬æ¢åçš„æ•°å­—æˆ–é»˜è®¤å€¼
        """
        if value is None or value == "":
            return default

        try:
            str_value = str(value).strip()

            # å¤„ç†ç‰¹æ®Šå€¼
            if str_value.lower() in ["nan", "null", "none", "-", "--"]:
                return default

            # ç§»é™¤é€—å·åˆ†éš”ç¬¦
            str_value = str_value.replace(",", "")

            # å¤„ç†ä¸­æ–‡å•ä½
            multiplier = 1
            if "ä¸‡" in str_value:
                str_value = str_value.replace("ä¸‡", "")
                multiplier = SyncConstants.WAN_MULTIPLIER
            elif "äº¿" in str_value:
                str_value = str_value.replace("äº¿", "")
                multiplier = SyncConstants.YI_MULTIPLIER

            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            number = float(str_value)
            return number * multiplier

        except (ValueError, TypeError) as e:
            self.logger.debug(f"æ•°å­—è½¬æ¢å¤±è´¥: {value} -> {e}")
            return default

    def _safe_extract_date(
        self, value: Any, default: Optional[str] = None
    ) -> Optional[str]:
        """
        å®‰å…¨æå–æ—¥æœŸï¼Œç»Ÿä¸€æ ¼å¼åŒ–ä¸ºYYYY-MM-DD

        Args:
            value: å¾…è½¬æ¢çš„æ—¥æœŸå€¼
            default: é»˜è®¤å€¼

        Returns:
            æ ¼å¼åŒ–åçš„æ—¥æœŸå­—ç¬¦ä¸²æˆ–é»˜è®¤å€¼
        """
        if value is None or value == "":
            return default

        try:
            str_value = str(value).strip()

            # å¤„ç†ç‰¹æ®Šå€¼
            if str_value.lower() in ["nan", "null", "none", "-", "--"]:
                return default

            # åŒ¹é…å¸¸è§æ—¥æœŸæ ¼å¼
            date_patterns = [
                r"(\d{4})-(\d{1,2})-(\d{1,2})",  # YYYY-MM-DD
                r"(\d{4})/(\d{1,2})/(\d{1,2})",  # YYYY/MM/DD
                r"(\d{4})\.(\d{1,2})\.(\d{1,2})",  # YYYY.MM.DD
                r"(\d{4})(\d{2})(\d{2})",  # YYYYMMDD
            ]

            for pattern in date_patterns:
                match = re.match(pattern, str_value)
                if match:
                    year, month, day = match.groups()
                    # æ ‡å‡†åŒ–æ ¼å¼
                    try:
                        parsed_date = datetime(int(year), int(month), int(day))
                        return parsed_date.strftime(SyncConstants.DATE_FORMAT)
                    except ValueError:
                        continue

            self.logger.debug(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {value}")
            return default

        except (ValueError, TypeError) as e:
            self.logger.debug(f"æ—¥æœŸè½¬æ¢å¤±è´¥: {value} -> {e}")
            return default

    def _sync_extended_data(
        self, symbols: List[str], target_date: date, progress_bar=None
    ) -> Dict[str, Any]:
        """
        å¢é‡åŒæ­¥æ‰©å±•æ•°æ®ï¼ˆè´¢åŠ¡æ•°æ®ã€ä¼°å€¼æ•°æ®ç­‰ï¼‰

        ä¼˜åŒ–ç­–ç•¥ï¼š
        - è´¢åŠ¡æ•°æ®ï¼šä½¿ç”¨æ‰¹é‡å¯¼å…¥ï¼ˆä¸€æ¬¡æ€§è·å–æ‰€æœ‰è‚¡ç¥¨ï¼Œé¿å…é€ä¸ªæŸ¥è¯¢çš„å·¨å¤§å¼€é”€ï¼‰
        - ä¼°å€¼æ•°æ®ï¼šé€ä¸ªè·å–ï¼ˆæ•°æ®æºä¸æ”¯æŒæ‰¹é‡APIï¼‰
        """
        session_id = str(uuid.uuid4())
        self.logger.info(f"ğŸ”„ å¼€å§‹æ‰©å±•æ•°æ®åŒæ­¥: {len(symbols)}åªè‚¡ç¥¨")

        result = {
            "financials_count": 0,
            "valuations_count": 0,
            "indicators_count": 0,
            "processed_symbols": 0,
            "failed_symbols": 0,
            "session_id": session_id,
            "batch_mode": False,
        }

        if not symbols:
            self.logger.info("âœ… æ²¡æœ‰è‚¡ç¥¨éœ€è¦å¤„ç†")
            if progress_bar:
                progress_bar.update(0)
            return result

        self.logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†: {len(symbols)}åªè‚¡ç¥¨")

        # ğŸš€ ä¼˜åŒ–1: æ‰¹é‡å¯¼å…¥è´¢åŠ¡æ•°æ®ï¼ˆå½“è‚¡ç¥¨æ•°é‡>50æ—¶å¯ç”¨æ‰¹é‡æ¨¡å¼ï¼‰
        batch_threshold = 50
        financial_data_map = {}  # symbol -> financial_data

        if len(symbols) >= batch_threshold:
            self.logger.info(
                f"âš¡ æ£€æµ‹åˆ°æ‰¹é‡åœºæ™¯({len(symbols)}åªè‚¡ç¥¨)ï¼Œå¯ç”¨æ‰¹é‡è´¢åŠ¡æ•°æ®å¯¼å…¥"
            )
            result["batch_mode"] = True

            try:
                # è®¡ç®—æŠ¥å‘ŠæœŸï¼ˆä½¿ç”¨å»å¹´å¹´æŠ¥ï¼‰
                report_year = target_date.year - 1
                report_date_str = f"{report_year}-12-31"

                # æ‰¹é‡å¯¼å…¥æ‰€æœ‰è‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®
                self.logger.info(f"å¼€å§‹æ‰¹é‡å¯¼å…¥è´¢åŠ¡æ•°æ®: {report_date_str}")
                batch_result = self.data_source_manager.batch_import_financial_data(
                    report_date_str, "Q4"
                )

                # æ£€æŸ¥batch_resultæ˜¯å¦ä¸ºå­—å…¸ç±»å‹
                self.logger.debug(f"æ‰¹é‡å¯¼å…¥è¿”å›ç±»å‹: {type(batch_result)}")

                if not isinstance(batch_result, dict):
                    self.logger.warning(f"æ‰¹é‡å¯¼å…¥è¿”å›éå­—å…¸ç±»å‹: {type(batch_result)}")
                    result["batch_mode"] = False
                elif batch_result.get("success") and batch_result.get("data"):
                    # è§£åŒ…åµŒå¥—çš„æ•°æ®ç»“æ„ï¼ˆ@unified_error_handler å¯¼è‡´çš„åŒé‡åŒ…è£…ï¼‰
                    inner_data = batch_result.get("data")

                    if isinstance(inner_data, dict) and "data" in inner_data:
                        # åŒé‡åµŒå¥—: {'data': {'data': [...]}}
                        actual_records = inner_data["data"]
                        self.logger.debug(
                            f"è§£åŒ…åŒé‡åµŒå¥—æ•°æ®ç»“æ„ï¼Œè·å–åˆ° {len(actual_records) if isinstance(actual_records, list) else 0} æ¡è®°å½•"
                        )
                    else:
                        # å•å±‚åµŒå¥—: {'data': [...]}
                        actual_records = inner_data
                        self.logger.debug(
                            f"ä½¿ç”¨å•å±‚æ•°æ®ç»“æ„ï¼Œè·å–åˆ° {len(actual_records) if isinstance(actual_records, list) else 0} æ¡è®°å½•"
                        )

                    # éªŒè¯å®é™…è®°å½•æ˜¯å¦ä¸ºåˆ—è¡¨
                    if not isinstance(actual_records, list):
                        self.logger.warning(
                            f"æ‰¹é‡å¯¼å…¥æ•°æ®æ ¼å¼é”™è¯¯: actual_recordsä¸æ˜¯åˆ—è¡¨ï¼Œç±»å‹ä¸º{type(actual_records)}"
                        )
                        result["batch_mode"] = False
                    else:
                        # å¯¼å…¥å­—æ®µæ˜ å°„å‡½æ•°
                        try:
                            from simtradedata.data_sources.mootdx_finvalue_fields import (
                                map_financial_data,
                            )

                            has_mapper = True
                        except ImportError:
                            self.logger.warning(
                                "æœªæ‰¾åˆ°mootdxå­—æ®µæ˜ å°„æ¨¡å—ï¼Œä½¿ç”¨åŸå§‹æ•°æ®"
                            )
                            has_mapper = False

                        # æ„å»ºsymbol -> dataæ˜ å°„
                        self.logger.debug(
                            f"å¼€å§‹æ„å»ºè´¢åŠ¡æ•°æ®æ˜ å°„ï¼Œsymbolsæ•°é‡: {len(symbols)}, recordsæ•°é‡: {len(actual_records)}"
                        )

                        for record in actual_records:
                            symbol = record.get("symbol")
                            if symbol in symbols:  # åªå¤„ç†éœ€è¦åŒæ­¥çš„è‚¡ç¥¨
                                raw_data = record.get("data", {})

                                # åº”ç”¨å­—æ®µæ˜ å°„ï¼ˆå°†é€šè¾¾ä¿¡åˆ—åæ˜ å°„ä¸ºæ ‡å‡†å­—æ®µåï¼‰
                                if has_mapper and raw_data:
                                    try:
                                        mapped_data = map_financial_data(raw_data)
                                    except Exception as e:
                                        self.logger.warning(
                                            f"å­—æ®µæ˜ å°„å¤±è´¥ {symbol}: {e}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®"
                                        )
                                        mapped_data = raw_data
                                else:
                                    mapped_data = raw_data

                                financial_data_map[symbol] = {
                                    "data": mapped_data,
                                    "report_date": record.get("report_date"),
                                    "report_type": record.get("report_type"),
                                }

                        self.logger.info(
                            f"âœ… æ‰¹é‡å¯¼å…¥å®Œæˆ: è·å–åˆ° {len(financial_data_map)} åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®"
                        )
                else:
                    self.logger.warning(f"æ‰¹é‡å¯¼å…¥å¤±è´¥ï¼Œå°†å›é€€åˆ°é€ä¸ªæŸ¥è¯¢æ¨¡å¼")
                    result["batch_mode"] = False

            except Exception as e:
                self.logger.error(f"æ‰¹é‡å¯¼å…¥è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
                self.logger.warning("å°†å›é€€åˆ°é€ä¸ªæŸ¥è¯¢æ¨¡å¼")
                result["batch_mode"] = False

        # å¤„ç†æ¯åªè‚¡ç¥¨çš„æ‰©å±•æ•°æ®
        self.logger.debug(f"å¼€å§‹é€åªå¤„ç†è‚¡ç¥¨ï¼Œæ‰¹é‡æ¨¡å¼: {result.get('batch_mode')}")
        for i, symbol in enumerate(symbols):
            self.logger.debug(f"å¤„ç† {symbol} ({i+1}/{len(symbols)})")

            try:
                # å¦‚æœæ‰¹é‡æ¨¡å¼æˆåŠŸï¼Œä¼ å…¥é¢„åŠ è½½çš„è´¢åŠ¡æ•°æ®
                preloaded_financial = financial_data_map.get(symbol)

                # ä½¿ç”¨äº‹åŠ¡ä¿æŠ¤åŒæ­¥å•ä¸ªè‚¡ç¥¨
                symbol_result = self._sync_single_symbol_with_transaction(
                    symbol, target_date, session_id, preloaded_financial
                )

                # æ›´æ–°ç»“æœç»Ÿè®¡
                if symbol_result.get("success", False):
                    result["financials_count"] += symbol_result.get(
                        "financials_count", 0
                    )
                    result["valuations_count"] += symbol_result.get(
                        "valuations_count", 0
                    )
                    result["indicators_count"] += symbol_result.get(
                        "indicators_count", 0
                    )
                else:
                    result["failed_symbols"] += 1

                result["processed_symbols"] += 1

            except Exception as e:
                self.logger.error(f"åŒæ­¥è‚¡ç¥¨å¤±è´¥: {symbol} - {e}")
                self.logger.debug(f"åŒæ­¥è‚¡ç¥¨è¯¦ç»†é”™è¯¯: {symbol}", exc_info=True)
                result["failed_symbols"] += 1
                result["processed_symbols"] += 1

            # æ›´æ–°è¿›åº¦æ¡
            if progress_bar:
                progress_bar.update(1)

        return result

    def _log_data_failure_with_context(
        self, symbol: str, target_date: date, failure_reasons: List[str]
    ):
        """
        è®°å½•å¸¦ä¸Šä¸‹æ–‡çš„æ•°æ®è·å–å¤±è´¥ä¿¡æ¯

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            target_date: ç›®æ ‡æ—¥æœŸ
            failure_reasons: å¤±è´¥åŸå› åˆ—è¡¨
        """
        try:
            # å°è¯•è·å–è‚¡ç¥¨ä¿¡æ¯æ¥åˆ¤æ–­æ˜¯å¦å·²ä¸Šå¸‚
            stock_info_result = self.data_source_manager.get_stock_info(symbol)

            # å¤„ç†å¤šå±‚åµŒå¥—çš„å“åº”æ ¼å¼
            stock_info = stock_info_result
            while isinstance(stock_info, dict) and "data" in stock_info:
                if stock_info.get("success") is False:
                    # å¦‚æœä»»ä½•å±‚çº§å¤±è´¥ï¼Œè·³å‡ºå¤„ç†
                    stock_info = None
                    break
                stock_info = stock_info["data"]

            if stock_info and isinstance(stock_info, dict):
                ipo_date_str = stock_info.get("list_date", "")
                if ipo_date_str:
                    try:
                        ipo_date = datetime.strptime(
                            ipo_date_str, SyncConstants.DATE_FORMAT
                        ).date()

                        if ipo_date > target_date:
                            # è‚¡ç¥¨å°šæœªä¸Šå¸‚ï¼Œè¿™æ˜¯é¢„æœŸæƒ…å†µ
                            self.logger.info(
                                f"è‚¡ç¥¨å°šæœªä¸Šå¸‚: {symbol} (ä¸Šå¸‚æ—¥æœŸ: {ipo_date_str}, ç›®æ ‡æ—¥æœŸ: {target_date})"
                            )
                            return
                        else:
                            # è‚¡ç¥¨å·²ä¸Šå¸‚ä½†æ•°æ®è·å–å¤±è´¥ï¼Œè¿™å¯èƒ½æ˜¯æ•°æ®æºé—®é¢˜æˆ–è‚¡ç¥¨çŠ¶æ€å¼‚å¸¸
                            failure_detail = ", ".join(failure_reasons)
                            self.logger.warning(
                                f"æ•°æ®è·å–å¤±è´¥: {symbol} ({failure_detail}) - è‚¡ç¥¨å·²ä¸Šå¸‚ä½†æ— å¯ç”¨æ•°æ®"
                            )
                            return
                    except (ValueError, TypeError):
                        # IPOæ—¥æœŸæ ¼å¼é”™è¯¯ï¼ŒæŒ‰ä¸€èˆ¬å¤±è´¥å¤„ç†
                        pass

            # æ— æ³•è·å–IPOä¿¡æ¯æˆ–æ ¼å¼å¼‚å¸¸ï¼ŒæŒ‰ä¸€èˆ¬æ•°æ®è·å–å¤±è´¥å¤„ç†
            failure_detail = ", ".join(failure_reasons)
            self.logger.warning(f"æ•°æ®è·å–å¤±è´¥: {symbol} ({failure_detail})")

        except Exception as e:
            # è·å–è‚¡ç¥¨ä¿¡æ¯æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œè®°å½•è¯¦ç»†é”™è¯¯
            failure_detail = ", ".join(failure_reasons)
            self.logger.warning(f"æ•°æ®è·å–å¤±è´¥: {symbol} ({failure_detail})")
            self.logger.debug(f"è‚¡ç¥¨ä¿¡æ¯æ£€æŸ¥å¼‚å¸¸: {symbol} - {e}")

    def _sync_single_symbol_with_transaction(
        self,
        symbol: str,
        target_date: date,
        session_id: str,
        preloaded_financial: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨äº‹åŠ¡ä¿æŠ¤åŒæ­¥å•ä¸ªè‚¡ç¥¨çš„æ‰©å±•æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            target_date: ç›®æ ‡æ—¥æœŸ
            session_id: ä¼šè¯ID
            preloaded_financial: é¢„åŠ è½½çš„è´¢åŠ¡æ•°æ®ï¼ˆæ‰¹é‡æ¨¡å¼ä¸‹ä¼ å…¥ï¼‰
        """
        result = {
            "success": False,
            "financials_count": 0,
            "valuations_count": 0,
            "indicators_count": 0,
        }

        try:
            # å¼€å§‹äº‹åŠ¡
            self.db_manager.execute("BEGIN TRANSACTION")

            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™åªè‚¡ç¥¨
            existing_status = self.db_manager.fetchone(
                "SELECT status FROM extended_sync_status WHERE symbol = ? AND target_date = ? AND status = 'completed'",
                (symbol, str(target_date)),
            )

            if existing_status:
                self.logger.debug(f"â­ï¸ è·³è¿‡å·²å®Œæˆçš„è‚¡ç¥¨: {symbol}")
                result["success"] = True
                self.db_manager.execute("COMMIT")
                return result

            # æ ‡è®°å¼€å§‹å¤„ç†
            self.db_manager.execute(
                "INSERT OR REPLACE INTO extended_sync_status (symbol, sync_type, target_date, status, session_id, created_at, updated_at) VALUES (?, ?, ?, ?, ?, datetime('now'), datetime('now'))",
                (symbol, "processing", str(target_date), "processing", session_id),
            )

            # æ•°æ®è·å–æˆåŠŸæ ‡å¿—
            financial_success = False
            valuation_success = False

            # å¤„ç†è´¢åŠ¡æ•°æ®
            report_year = target_date.year - 1  # ä½¿ç”¨å»å¹´å¹´æŠ¥
            report_date_str = f"{report_year}-12-31"

            # éªŒè¯æŠ¥å‘ŠæœŸæœ‰æ•ˆæ€§
            if DataQualityValidator.is_valid_report_date(report_date_str, symbol):
                try:
                    # ğŸš€ ä¼˜åŒ–: ä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„è´¢åŠ¡æ•°æ®ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
                    if preloaded_financial and preloaded_financial.get("data"):
                        self.logger.debug(f"ä½¿ç”¨é¢„åŠ è½½çš„è´¢åŠ¡æ•°æ®: {symbol}")
                        financial_data = preloaded_financial["data"]
                        data_source = "mootdx_batch"  # æ ‡è®°ä¸ºæ‰¹é‡å¯¼å…¥
                    else:
                        # å›é€€: é€ä¸ªæŸ¥è¯¢ï¼ˆå•è‚¡æ¨¡å¼æˆ–æ‰¹é‡å¤±è´¥æ—¶ï¼‰
                        self.logger.debug(f"é€ä¸ªæŸ¥è¯¢è´¢åŠ¡æ•°æ®: {symbol}")
                        financial_result = self.data_source_manager.get_fundamentals(
                            symbol, report_date_str, "Q4"
                        )

                        # æ ‡å‡†æ•°æ®æºå“åº”æ ¼å¼è§£åŒ…
                        financial_data = self._extract_data_safely(financial_result)

                        # è·å–æ•°æ®æ¥æº
                        data_source = (
                            financial_result.get("source", "unknown")
                            if isinstance(financial_result, dict)
                            else "unknown"
                        )

                    # ä½¿ç”¨æ”¾å®½çš„éªŒè¯æ ‡å‡†
                    if financial_data and self._is_valid_financial_data_relaxed(
                        financial_data
                    ):
                        self._insert_financial_data(
                            financial_data, symbol, report_date_str, data_source
                        )
                        result["financials_count"] += 1
                        financial_success = True
                        self.logger.debug(f"{data_source}è´¢åŠ¡æ•°æ®æ’å…¥æˆåŠŸ: {symbol}")
                    else:
                        self.logger.debug(f"è´¢åŠ¡æ•°æ®æ— æ•ˆ: {symbol}")

                except Exception as e:
                    self.logger.warning(f"è·å–è´¢åŠ¡æ•°æ®å¤±è´¥: {symbol} - {e}")
            else:
                self.logger.warning(f"è·³è¿‡æ— æ•ˆæŠ¥å‘ŠæœŸ: {symbol} {report_date_str}")

            # å¤„ç†ä¼°å€¼æ•°æ®
            try:
                # ä½¿ç”¨DataSourceManagerç»Ÿä¸€è·å–ä¼°å€¼æ•°æ®ï¼ˆæ ¹æ®ä¼˜å…ˆçº§é…ç½®ï¼‰
                valuation_result = self.data_source_manager.get_valuation_data(
                    symbol, str(target_date)
                )

                # æ ‡å‡†æ•°æ®æºå“åº”æ ¼å¼è§£åŒ…
                valuation_data = self._extract_data_safely(valuation_result)

                # è·å–æ•°æ®æ¥æº
                data_source = (
                    valuation_result.get("source", "unknown")
                    if isinstance(valuation_result, dict)
                    else "unknown"
                )

                # éªŒè¯ä¼°å€¼æ•°æ®æœ‰æ•ˆæ€§
                if valuation_data and DataQualityValidator.is_valid_valuation_data(
                    valuation_data
                ):
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥è®°å½•
                    record_date = valuation_data.get("date", str(target_date))
                    existing = self.db_manager.fetchone(
                        "SELECT COUNT(*) as count FROM valuations WHERE symbol = ? AND date = ?",
                        (symbol, record_date),
                    )

                    if existing and existing["count"] == 0:
                        self.db_manager.execute(
                            """INSERT INTO valuations
                            (symbol, date, pe_ratio, pb_ratio, ps_ratio, pcf_ratio, source, created_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?, datetime('now'))""",
                            (
                                symbol,
                                record_date,
                                valuation_data.get("pe_ratio"),
                                valuation_data.get("pb_ratio"),
                                valuation_data.get("ps_ratio"),
                                valuation_data.get("pcf_ratio"),
                                data_source,
                            ),
                        )
                        result["valuations_count"] += 1
                        valuation_success = True
                        self.logger.debug(f"{data_source}ä¼°å€¼æ•°æ®æ’å…¥æˆåŠŸ: {symbol}")
                    else:
                        self.logger.debug(f"ä¼°å€¼æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡: {symbol}")
                else:
                    self.logger.debug(f"ä¼°å€¼æ•°æ®æ— æ•ˆ: {symbol}")

            except Exception as e:
                self.logger.warning(f"è·å–ä¼°å€¼æ•°æ®å¤±è´¥: {symbol} - {e}")

            # å¤„ç†æŠ€æœ¯æŒ‡æ ‡ï¼ˆæš‚æ—¶è·³è¿‡ï¼Œæ ‡è®°ä¸ºæˆåŠŸï¼‰

            # æ ¹æ®æ•°æ®è·å–ç»“æœå†³å®šæœ€ç»ˆçŠ¶æ€ï¼ˆä½¿ç”¨åˆ†çº§æ ‡å‡†ï¼‰
            failure_reasons = []

            if financial_success:
                final_status = "completed"  # æœ‰è´¢åŠ¡æ•°æ®å°±ç®—å®Œæˆ
                result["success"] = True
                self.logger.debug(
                    f"æ•°æ®è·å–æˆåŠŸ: {symbol} (è´¢åŠ¡:{financial_success}, ä¼°å€¼:{valuation_success})"
                )
            elif valuation_success:
                final_status = "partial"  # åªæœ‰ä¼°å€¼æ•°æ®ç®—éƒ¨åˆ†å®Œæˆ
                result["success"] = True
                self.logger.debug(f"éƒ¨åˆ†æ•°æ®è·å–æˆåŠŸ: {symbol} (ä»…ä¼°å€¼æ•°æ®)")
            else:
                final_status = "failed"
                result["success"] = False

                # æ”¶é›†å…·ä½“çš„å¤±è´¥åŸå› 
                if not financial_success:
                    failure_reasons.append("è´¢åŠ¡æ•°æ®")
                if not valuation_success:
                    failure_reasons.append("ä¼°å€¼æ•°æ®")

                # æ£€æŸ¥è‚¡ç¥¨ä¸Šå¸‚çŠ¶æ€å’Œå¤±è´¥åŸå› 
                self._log_data_failure_with_context(
                    symbol, target_date, failure_reasons
                )

            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
            self.db_manager.execute(
                "UPDATE extended_sync_status SET status = ?, updated_at = datetime('now') WHERE symbol = ? AND target_date = ? AND session_id = ?",
                (final_status, symbol, str(target_date), session_id),
            )

            # æäº¤äº‹åŠ¡
            self.db_manager.execute("COMMIT")
            return result

        except Exception as e:
            # å›æ»šäº‹åŠ¡
            try:
                self.db_manager.execute("ROLLBACK")
            except:
                pass  # å¿½ç•¥å›æ»šé”™è¯¯
            self.logger.error(f"åŒæ­¥è‚¡ç¥¨å¤±è´¥ï¼Œäº‹åŠ¡å›æ»š: {symbol} - {e}")
            result["success"] = False
            return result

    def _auto_fix_gaps(self, gap_result: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªåŠ¨ä¿®å¤ç¼ºå£"""
        self.logger.info("å¼€å§‹è‡ªåŠ¨ä¿®å¤ç¼ºå£")

        fix_result = {
            "total_gaps": gap_result["summary"]["total_gaps"],
            "attempted_fixes": 0,
            "successful_fixes": 0,
            "failed_fixes": 0,
            "skipped_fixes": 0,  # æ–°å¢ï¼šè·³è¿‡çš„ä¿®å¤
        }

        # å¤„ç†ç¼ºå£æ•°æ®ç»“æ„ - é€‚é…æ–°çš„æ•°æ®æ ¼å¼
        all_gaps = []
        for freq_data in gap_result.get("gaps_by_frequency", {}).values():
            all_gaps.extend(freq_data.get("gaps", []))

        if not all_gaps:
            self.logger.info("æ²¡æœ‰å‘ç°ç¼ºå£ï¼Œæ— éœ€ä¿®å¤")
            return fix_result

        # é™åˆ¶ä¿®å¤æ•°é‡ï¼Œä¼˜å…ˆä¿®å¤é‡è¦è‚¡ç¥¨çš„ç¼ºå£
        max_fixes = 10
        fixes_attempted = 0

        for gap in all_gaps:
            if fixes_attempted >= max_fixes:
                break

            symbol = gap.get("symbol")
            gap_start = gap.get("start_date")
            gap_end = gap.get("end_date")
            frequency = gap.get("frequency", "1d")

            if not symbol or not gap_start or not gap_end or frequency != "1d":
                continue

            # æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦é€‚åˆä¿®å¤ï¼ˆé¿å…ä¿®å¤æ–°è‚¡æˆ–åœç‰Œè‚¡çš„ç¼ºå£ï¼‰
            stock_info = self.db_manager.fetchone(
                "SELECT list_date, status FROM stocks WHERE symbol = ?", (symbol,)
            )

            if not stock_info:
                self.logger.debug(f"è·³è¿‡ä¿®å¤: {symbol} - è‚¡ç¥¨ä¿¡æ¯ä¸å­˜åœ¨")
                continue

            # æ£€æŸ¥ç¼ºå£æ˜¯å¦åœ¨è‚¡ç¥¨ä¸Šå¸‚æ—¥æœŸä¹‹å
            if stock_info["list_date"]:
                from datetime import datetime

                list_date = datetime.strptime(
                    stock_info["list_date"], "%Y-%m-%d"
                ).date()
                gap_start_date = datetime.strptime(gap_start, "%Y-%m-%d").date()

                if gap_start_date < list_date:
                    fix_result["skipped_fixes"] += 1
                    self.logger.debug(f"è·³è¿‡ä¿®å¤: {symbol} ç¼ºå£æ—¥æœŸæ—©äºä¸Šå¸‚æ—¥æœŸ")
                    continue

            fix_result["attempted_fixes"] += 1
            fixes_attempted += 1

            self.logger.info(f"ä¿®å¤ç¼ºå£: {symbol} {gap_start} åˆ° {gap_end}")

            # è·å–æ•°æ®å¡«è¡¥ç¼ºå£
            daily_data = self.data_source_manager.get_daily_data(
                symbol, gap_start, gap_end
            )

            if isinstance(daily_data, dict) and "data" in daily_data:
                daily_data = daily_data["data"]

            # å®é™…å¤„ç†æ•°æ®æ’å…¥
            if (
                daily_data is not None
                and hasattr(daily_data, "__len__")
                and len(daily_data) > 0
            ):
                try:
                    # ä½¿ç”¨å¤„ç†å¼•æ“æ’å…¥ç¼ºå£æ•°æ®
                    processed_result = self.processing_engine.process_symbol_data(
                        symbol, str(gap_start), str(gap_end), frequency
                    )
                    records_inserted = processed_result.get("records", 0)

                    if records_inserted > 0:
                        fix_result["successful_fixes"] += 1
                        self.logger.info(
                            f"ç¼ºå£ä¿®å¤æˆåŠŸ: {symbol} æ’å…¥{records_inserted}æ¡è®°å½•"
                        )
                    else:
                        fix_result["failed_fixes"] += 1
                        self.logger.warning(
                            f"ç¼ºå£ä¿®å¤å¤±è´¥: {symbol} å¤„ç†å¼•æ“æœªæ’å…¥æ•°æ®"
                        )
                except Exception as e:
                    fix_result["failed_fixes"] += 1
                    self.logger.warning(f"ç¼ºå£ä¿®å¤å‡ºé”™: {symbol} - {e}")
            else:
                fix_result["failed_fixes"] += 1
                self.logger.debug(f"ç¼ºå£ä¿®å¤è·³è¿‡: {symbol} æ•°æ®æºæ— æ•°æ®ï¼ˆå¯èƒ½æ­£å¸¸ï¼‰")

        self.logger.info(
            f"ç¼ºå£ä¿®å¤å®Œæˆ: å°è¯•={fix_result['attempted_fixes']}, æˆåŠŸ={fix_result['successful_fixes']}, å¤±è´¥={fix_result['failed_fixes']}, è·³è¿‡={fix_result['skipped_fixes']}"
        )

        # å¦‚æœå¤§éƒ¨åˆ†ç¼ºå£éƒ½æ— æ³•ä¿®å¤ï¼Œè¯´æ˜è¿™äº›ç¼ºå£å¯èƒ½æ˜¯æ­£å¸¸çš„
        if fix_result["attempted_fixes"] > 0:
            success_rate = (
                fix_result["successful_fixes"] / fix_result["attempted_fixes"]
            )
            if success_rate < 0.3:
                self.logger.info(
                    "ğŸ’¡ å¤§éƒ¨åˆ†ç¼ºå£æ— æ³•ä¿®å¤ï¼Œè¿™å¯èƒ½æ˜¯æ­£å¸¸ç°è±¡ï¼ˆæ–°è‚¡ã€åœç‰Œç­‰ï¼‰"
                )

        return fix_result

    def _is_valid_financial_data_relaxed(self, data: Dict[str, Any]) -> bool:
        """æ”¾å®½çš„è´¢åŠ¡æ•°æ®æœ‰æ•ˆæ€§éªŒè¯"""
        if not data or not isinstance(data, dict):
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æœ‰æ•ˆçš„è´¢åŠ¡æŒ‡æ ‡ï¼ˆæ”¾å®½æ ‡å‡†ï¼‰
        revenue = data.get("revenue", 0)
        net_profit = data.get("net_profit", 0)
        total_assets = data.get("total_assets", 0)
        shareholders_equity = data.get("shareholders_equity", 0)
        eps = data.get("eps", 0)

        # åªè¦æœ‰ä¸€ä¸ªéç©º/éé›¶çš„è´¢åŠ¡æŒ‡æ ‡å°±è®¤ä¸ºæœ‰æ•ˆ
        return (
            (revenue and revenue != 0)
            or (total_assets and total_assets != 0)
            or (shareholders_equity and shareholders_equity != 0)
            or (net_profit != 0)  # å‡€åˆ©æ¶¦å¯ä»¥ä¸ºè´Ÿ
            or (eps and eps != 0)  # æ¯è‚¡æ”¶ç›Šå¯ä»¥ä¸ºè´Ÿ
        )

    def _insert_financial_data(
        self,
        financial_data: Dict[str, Any],
        symbol: str,
        report_date_str: str,
        source: str,
    ):
        """æ’å…¥è´¢åŠ¡æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            self.db_manager.execute(
                """INSERT OR REPLACE INTO financials (
                    symbol, report_date, report_type, revenue, operating_profit, net_profit,
                    gross_margin, net_margin, total_assets, total_liabilities, shareholders_equity,
                    operating_cash_flow, investing_cash_flow, financing_cash_flow,
                    eps, bps, roe, roa, debt_ratio, source, created_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))""",
                (
                    symbol,
                    report_date_str,
                    "Q4",
                    financial_data.get("revenue", 0),
                    financial_data.get("operating_profit", 0),
                    financial_data.get("net_profit", 0),
                    financial_data.get("gross_margin", 0),
                    financial_data.get("net_margin", 0),
                    financial_data.get("total_assets", 0),
                    financial_data.get("total_liabilities", 0),
                    financial_data.get("shareholders_equity", 0),
                    financial_data.get("operating_cash_flow", 0),
                    financial_data.get("investing_cash_flow", 0),
                    financial_data.get("financing_cash_flow", 0),
                    financial_data.get("eps", 0),
                    financial_data.get("bps", 0),
                    financial_data.get("roe", 0),
                    financial_data.get("roa", 0),
                    financial_data.get("debt_ratio", 0),
                    source,
                ),
            )
        except Exception as e:
            self.logger.error(f"æ’å…¥è´¢åŠ¡æ•°æ®å¤±è´¥ {symbol}: {e}")
            raise

    def generate_sync_report(self, full_result: Dict[str, Any]) -> str:
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        report_lines = []

        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.append("=" * 60)
        report_lines.append("æ•°æ®åŒæ­¥æŠ¥å‘Š")
        report_lines.append("=" * 60)
        report_lines.append(f"åŒæ­¥æ—¶é—´: {full_result.get('start_time', '')}")
        report_lines.append(f"ç›®æ ‡æ—¥æœŸ: {full_result.get('target_date', '')}")
        report_lines.append(f"æ€»è€—æ—¶: {full_result.get('duration_seconds', 0):.2f} ç§’")
        report_lines.append("")

        # é˜¶æ®µæ±‡æ€»
        summary = full_result.get("summary", {})
        report_lines.append("é˜¶æ®µæ±‡æ€»:")
        report_lines.append(f"  æ€»é˜¶æ®µæ•°: {summary.get('total_phases', 0)}")
        report_lines.append(f"  æˆåŠŸé˜¶æ®µ: {summary.get('successful_phases', 0)}")
        report_lines.append(f"  å¤±è´¥é˜¶æ®µ: {summary.get('failed_phases', 0)}")
        report_lines.append("")

        # å¢é‡åŒæ­¥è¯¦æƒ…
        phases = full_result.get("phases", {})
        if "incremental_sync" in phases:
            phase = phases["incremental_sync"]
            report_lines.append("å¢é‡åŒæ­¥:")
            report_lines.append(f"  çŠ¶æ€: {phase['status']}")

            if phase["status"] == "completed" and "result" in phase:
                result = phase["result"]
                report_lines.append(f"  æ€»è‚¡ç¥¨æ•°: {result.get('total_symbols', 0)}")
                report_lines.append(f"  æˆåŠŸæ•°é‡: {result.get('success_count', 0)}")
                report_lines.append(f"  é”™è¯¯æ•°é‡: {result.get('error_count', 0)}")
            elif "error" in phase:
                report_lines.append(f"  é”™è¯¯: {phase['error']}")

        return "\n".join(report_lines)
